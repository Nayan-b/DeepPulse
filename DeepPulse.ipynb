{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielray54/DeepPulse/blob/main/DeepPulse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggw0W9-jwABw"
      },
      "source": [
        "# DeepPulse\n",
        "## An Uncertainty-aware Deep Neural Network for Heart Rate Estimations from Wrist-worn Photoplethysmography\n",
        "> 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)\n",
        "\n",
        "- [Paper](https://ieeexplore.ieee.org/document/9871813) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages"
      ],
      "metadata": {
        "id": "jOKj__n6UAnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GlvbszNNwY60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7df79b-9a15-4432-b4be-42f52dda09c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.7)\n",
            "Requirement already satisfied: uncertainty-toolbox in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (1.21.5)\n",
            "Requirement already satisfied: pytest>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (7.2.0)\n",
            "Requirement already satisfied: black>=19.10b0 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (23.1a1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (1.7.3)\n",
            "Requirement already satisfied: tqdm>=4.54.0 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (4.62.3)\n",
            "Requirement already satisfied: shapely>=1.6.4.post2 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (1.8.1.post1)\n",
            "Requirement already satisfied: scikit-learn>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from uncertainty-toolbox) (1.0.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (4.4.0)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (2.6.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (1.5.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (2.0.1)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (0.10.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->uncertainty-toolbox) (0.4.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black>=19.10b0->uncertainty-toolbox) (4.11.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->uncertainty-toolbox) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->uncertainty-toolbox) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->uncertainty-toolbox) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->uncertainty-toolbox) (2.8.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4.3->uncertainty-toolbox) (1.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4.3->uncertainty-toolbox) (21.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4.3->uncertainty-toolbox) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4.3->uncertainty-toolbox) (21.3)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4.3->uncertainty-toolbox) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black>=19.10b0->uncertainty-toolbox) (3.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->uncertainty-toolbox) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->uncertainty-toolbox) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->uncertainty-toolbox) (1.1.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install uncertainty-toolbox\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Datasets"
      ],
      "metadata": {
        "id": "iLDPNneWTOXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BAMI\n",
        "!gdown 1g5gqh6vekEdi3ZT21Cdu_1fkfNjBeUOA"
      ],
      "metadata": {
        "id": "U8m6tEVDTENo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f430463-aa57-4c15-9b0a-46c0e76b893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g5gqh6vekEdi3ZT21Cdu_1fkfNjBeUOA\n",
            "To: /content/BAMI_Data\n",
            "\r  0% 0.00/16.2M [00:00<?, ?B/s]\r 84% 13.6M/16.2M [00:00<00:00, 136MB/s]\r100% 16.2M/16.2M [00:00<00:00, 149MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DALIA\n",
        "!gdown 12DnrzMCV_otfU5_YUbedwRRcIyiHShIm"
      ],
      "metadata": {
        "id": "gutgm2TBTWVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ac39aa-05fb-40e0-ffb2-0dbb8996c00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12DnrzMCV_otfU5_YUbedwRRcIyiHShIm\n",
            "To: /content/DaLia_Data\n",
            "100% 900M/900M [00:04<00:00, 188MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IEEE Test\n",
        "!gdown 1PSciZgnXPlsYBMzR1Oj4TjcTk_LjL7TQ\n",
        "\n",
        "#IEEE Train\n",
        "!gdown 174KyqOiuhl3Prsrn29KgeMmIJVgYLSQK"
      ],
      "metadata": {
        "id": "1RvxpJXZTWM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2012a10f-03bb-4753-e6fc-71d53c389ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PSciZgnXPlsYBMzR1Oj4TjcTk_LjL7TQ\n",
            "To: /content/IEEE_Test_Data\n",
            "100% 13.6M/13.6M [00:00<00:00, 98.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=174KyqOiuhl3Prsrn29KgeMmIJVgYLSQK\n",
            "To: /content/IEEE_Train_Data\n",
            "100% 21.7M/21.7M [00:00<00:00, 133MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Wr4RBLTzUhgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PAcl7MIQwlBS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8534932e-026a-49d7-dbf0-a913d99509b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version:  2.8.0\n",
            "Built with CUDA:  True\n",
            "CPUs:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
            "GPUs:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "#Import Necessary Packages\n",
        "#util libs\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import gc\n",
        "#dl libs\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import tensorflow.keras as keras\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import regularizers\n",
        "#deeplearning addtional libs\n",
        "import tensorflow_probability as tfp \n",
        "tfd = tfp.distributions\n",
        "import uncertainty_toolbox as uct\n",
        "#computation libs\n",
        "import scipy.signal as sci_sig\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import preprocessing \n",
        "from sklearn.model_selection import train_test_split\n",
        "import fnmatch\n",
        "import itertools\n",
        "import re\n",
        "from pathlib import Path\n",
        "#display libs\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import matplotlib as mpl\n",
        "from matplotlib.transforms import Bbox\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm_notebook, tqdm, trange\n",
        "from google.colab import output\n",
        "\n",
        "print(\"Tensorflow Version: \", tf.__version__)\n",
        "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
        "print(\"CPUs: \", tf.config.list_physical_devices('CPU'))\n",
        "print(\"GPUs: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJK8zoxfNDhe"
      },
      "source": [
        "### Experiment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ckip58rtBYd_"
      },
      "outputs": [],
      "source": [
        "########### PREPROCESSING VARIABLES ##################\n",
        "BAND_PASS_LOW = 0.5\n",
        "BAND_PASS_HIGH = 4.5\n",
        "BAND_PASS_ORDER = 2\n",
        "RESAMPLE_FS = 64\n",
        "\n",
        "########### ARCHITECTURAL VARIABLES ##################\n",
        "MODEL_TYPE = [\"Original\", \"Optimised\"]\n",
        "SENSOR_FILTER_SIZE = 16\n",
        "SENSOR_NUM_FILTER = 64\n",
        "SENSOR_DROPOUT = 0.15 \n",
        "GLOBAL_FILTER_SIZE = 16\n",
        "GLOBAL_NUM_FILTER = 128\n",
        "GLOBAL_DROPOUT = 0.15\n",
        "RECURRENT_UNITS = 32\n",
        "RECURRENT_DROPOUT = 0.15\n",
        "POOLING = 2\n",
        "\n",
        "########### TRAINING VARIABLES ##################\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "STRATIFIED = True\n",
        "TEST_SIZE= 0.33\n",
        "NUM_PREDICT_SAMPLES = 25\n",
        "OPTIMISER = tf.optimizers.Nadam()\n",
        "EXP_NAME = [\"DALIA\", \"BAMI\", \"IEEE\"]\n",
        "PATH = '/content/drive/MyDrive/DeepPulse/Exp/'\n",
        "\n",
        "########### OUTPUT VARIABLES ##################\n",
        "DISPLAY = False\n",
        "PUBLICATION = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### DATA ##################\n",
        "infile = open(\"/content/BAMI_Data\",'rb')\n",
        "BAMI_Data = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/content/DaLia_Data\",'rb')\n",
        "DaLia_Data = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/content/IEEE_Test_Data\",'rb')\n",
        "IEEE_Test_Data = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open(\"/content/IEEE_Train_Data\",'rb')\n",
        "IEEE_Train_Data = pickle.load(infile)\n",
        "infile.close()\n",
        "IEEE_Data = IEEE_Train_Data + IEEE_Test_Data\n",
        "DATA = [DaLia_Data, BAMI_Data, IEEE_Data]"
      ],
      "metadata": {
        "id": "oD6pM5odUomV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_DeepPulse_Org(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                   GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, \n",
        "                   GLOBAL_DROPOUT, RECURRENT_UNITS, \n",
        "                   RECURRENT_DROPOUT, MAXPOOLING):\n",
        "  keras.backend.clear_session()\n",
        "  #=-=================== INPUTS ================================\n",
        "  input_PPG = keras.Input(shape=(512, 1), name=\"PPG_Sigs\")\n",
        "  input_ACC = keras.Input(shape=(512, 3), name=\"ACC_Sigs\")\n",
        "  \n",
        "  #=-=================== PPG ================================\n",
        "  PPG_SENSOR_CONV_1 = Convolution_MaxPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, MAXPOOLING, SENSOR_DROPOUT, \n",
        "                                  input_PPG, \"PPG_SENSOR_1\")\n",
        "  PPG_SENSOR_CONV_2 = Convolution_MaxPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, MAXPOOLING, SENSOR_DROPOUT, \n",
        "                              PPG_SENSOR_CONV_1, \"PPG_SENSOR_2\")\n",
        "\n",
        "  #=-=================== ACC ================================\n",
        "  ACC_SENSOR_CONV_1 = Convolution_MaxPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, MAXPOOLING, SENSOR_DROPOUT,\n",
        "                                  input_ACC, \"ACC_SENSOR_1\")\n",
        "  ACC_SENSOR_CONV_2 = Convolution_MaxPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, MAXPOOLING, SENSOR_DROPOUT, \n",
        "                              ACC_SENSOR_CONV_1, \"ACC_SENSOR_2\")\n",
        "  \n",
        "  #=-=================== MERGE ================================\n",
        "  MERGE = layers.Concatenate(axis=2, name=\"MERGE\")([PPG_SENSOR_CONV_2, \n",
        "                                                    ACC_SENSOR_CONV_2])\n",
        "\n",
        "  #=-=================== GLOBAL ================================\n",
        "  GLOBAL_CONV_1 = Convolution_MaxPool_DropOut(GLOBAL_NUM_FILTER, \n",
        "                              GLOBAL_FILTER_SIZE, MAXPOOLING, SENSOR_DROPOUT, \n",
        "                              MERGE, \"GLOBAL_1\")\n",
        "  \n",
        "  GLOBAL_CONV_2 = Convolution_DropOut(GLOBAL_NUM_FILTER, \n",
        "                                              GLOBAL_FILTER_SIZE,\n",
        "                                              GLOBAL_DROPOUT, GLOBAL_CONV_1, \n",
        "                                              \"GLOBAL_2\")\n",
        "    \n",
        "  #=-==========================================================\n",
        "  RECURRENT_1 = LSTM(RECURRENT_UNITS, RECURRENT_DROPOUT, True, \n",
        "                     GLOBAL_CONV_2, \"RECURRENT_1\")\n",
        "  RECURRENT_2 = LSTM(RECURRENT_UNITS, RECURRENT_DROPOUT, True, \n",
        "                     RECURRENT_1, \"RECURRENT_2\")\n",
        "  #=-==========================================================\n",
        "  dist = Prediction_Conv_Dense(\"PREDICT\",RECURRENT_2)\n",
        "\n",
        "  DeepPulse = keras.Model(inputs = [input_PPG, input_ACC], outputs = dist, name=\"DeepPulse\")\n",
        "  return DeepPulse"
      ],
      "metadata": {
        "id": "1UkdnWBbDUCp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_DeepPulse_Opt(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                   GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, \n",
        "                   GLOBAL_DROPOUT, RECURRENT_UNITS, \n",
        "                   RECURRENT_DROPOUT, AVGPOOLING):\n",
        "  keras.backend.clear_session()\n",
        "  #=-=================== INPUTS ================================\n",
        "  input_PPG = keras.Input(shape=(512, 1), name=\"PPG_Sigs\")\n",
        "  input_ACC = keras.Input(shape=(512, 3), name=\"ACC_Sigs\")\n",
        "  \n",
        "  #=-=================== PPG ================================\n",
        "  PPG_SENSOR_CONV_1 = Convolution(SENSOR_NUM_FILTER, SENSOR_FILTER_SIZE, \n",
        "                                  input_PPG, \"PPG_SENSOR_1\")\n",
        "  PPG_SENSOR_CONV_2 = Convolution_AvgPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, AVGPOOLING, SENSOR_DROPOUT, \n",
        "                              PPG_SENSOR_CONV_1, \"PPG_SENSOR_2\")\n",
        "\n",
        "  #=-=================== ACC ================================\n",
        "  ACC_SENSOR_CONV_1 = Convolution(SENSOR_NUM_FILTER, SENSOR_FILTER_SIZE,\n",
        "                                  input_ACC, \"ACC_SENSOR_1\")\n",
        "  ACC_SENSOR_CONV_2 = Convolution_AvgPool_DropOut(SENSOR_NUM_FILTER, \n",
        "                              SENSOR_FILTER_SIZE, AVGPOOLING, SENSOR_DROPOUT, \n",
        "                              ACC_SENSOR_CONV_1, \"ACC_SENSOR_2\")\n",
        "  \n",
        "  #=-=================== MERGE ================================\n",
        "  MERGE = layers.Concatenate(axis=2, name=\"MERGE\")([PPG_SENSOR_CONV_2, \n",
        "                                                    ACC_SENSOR_CONV_2])\n",
        "\n",
        "  #=-=================== GLOBAL ================================\n",
        "  GLOBAL_CONV_1 = Convolution_AvgPool_DropOut(GLOBAL_NUM_FILTER, \n",
        "                              GLOBAL_FILTER_SIZE, AVGPOOLING, GLOBAL_DROPOUT, \n",
        "                              MERGE, \"GLOBAL_1\")\n",
        "  \n",
        "  GLOBAL_CONV_2 = Convolution_AvgPool_DropOut(GLOBAL_NUM_FILTER, \n",
        "                                              GLOBAL_FILTER_SIZE, AVGPOOLING,\n",
        "                                              GLOBAL_DROPOUT, GLOBAL_CONV_1, \n",
        "                                              \"GLOBAL_2\")\n",
        "    \n",
        "  #=-==========================================================\n",
        "  RECURRENT_1 = LSTM_LayerNorm(RECURRENT_UNITS, RECURRENT_DROPOUT, True, \n",
        "                     GLOBAL_CONV_2, \"RECURRENT_1\")\n",
        "  RECURRENT_2 = LSTM_LayerNorm(RECURRENT_UNITS, RECURRENT_DROPOUT, False, \n",
        "                     RECURRENT_1, \"RECURRENT_2\")\n",
        "  #=-==========================================================\n",
        "  dist = Prediction_Dense(\"PREDICT\",RECURRENT_2)\n",
        "\n",
        "  DeepPulse = keras.Model(inputs = [input_PPG, input_ACC], outputs = dist, name=\"DeepPulse\")\n",
        "  return DeepPulse"
      ],
      "metadata": {
        "id": "eo20EzYWVz_S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph68GXX_Ptk4"
      },
      "source": [
        "# Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Convolution_MaxPool_DropOut(SENSOR_NUM_FILTER, SENSOR_FEAT_FILTER_SIZE, POOLING, \n",
        "                       DROP_RATE, INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Convolutional block with Maxpooling & Dropout.\n",
        "\n",
        "    Args:\n",
        "        SENSOR_NUM_FILTER: The number of filters in the Convolutional block.\n",
        "        SENSOR_FEAT_FILTER_SIZE: The filter size of the Convolutional block.\n",
        "        POOLING: The AVG pooling for the Convolutional block.\n",
        "        INPUT: The input to the Convolutional block.\n",
        "        NAME: The name of the Convolutional block.\n",
        "\n",
        "    Returns:\n",
        "        x: The output of the Convolutional block.\n",
        "    \"\"\"\n",
        "    x = layers.Conv1D(\n",
        "          SENSOR_NUM_FILTER,\n",
        "          SENSOR_FEAT_FILTER_SIZE,\n",
        "          use_bias=False,\n",
        "          name =  NAME +\"_conv\",\n",
        "          padding=\"same\")(INPUT)\n",
        "    x = layers.ReLU(name =  NAME + \"_relu\")(x)\n",
        "    x = layers.BatchNormalization(name =  NAME + \"_batch_norm\")(x)\n",
        "    x = layers.MaxPooling1D(POOLING, name =  NAME + \"_max_pool\")(x)\n",
        "    x = layers.Dropout(DROP_RATE, name =  NAME + \"_dropout\")(x, training=True)\n",
        "    return x\n",
        "\n",
        "def Convolution_AvgPool_DropOut(SENSOR_NUM_FILTER, SENSOR_FEAT_FILTER_SIZE, \n",
        "                                POOLING, DROP_RATE, INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Convolutional block with AvgPool & Dropout.\n",
        "\n",
        "    Args:\n",
        "        SENSOR_NUM_FILTER: The number of filters in the Convolutional block.\n",
        "        SENSOR_FEAT_FILTER_SIZE: The filter size of the Convolutional block.\n",
        "        DROP_RATE: The dropout rate for the Convolutional block.\n",
        "        POOLING: The AVG pooling for the Convolutional block.\n",
        "        INPUT: The input to the Convolutional block.\n",
        "        NAME: The name of the Convolutional block.\n",
        "\n",
        "    Returns:\n",
        "        x: The output of the Convolutional block.\n",
        "    \"\"\"\n",
        "    x = layers.Conv1D(\n",
        "          SENSOR_NUM_FILTER,\n",
        "          SENSOR_FEAT_FILTER_SIZE,\n",
        "          use_bias=False,\n",
        "          name =  NAME +\"_conv\",\n",
        "          padding=\"same\")(INPUT)\n",
        "    x = layers.ReLU(name =  NAME + \"_relu\")(x)\n",
        "    x = layers.BatchNormalization(name =  NAME + \"_batch_norm\")(x)\n",
        "    x = layers.AveragePooling1D(POOLING, name =  NAME + \"_max_pool\")(x)\n",
        "    x = layers.Dropout(DROP_RATE, name =  NAME + \"_dropout\")(x, training=True)\n",
        "    return x\n",
        "\n",
        "def Convolution_DropOut(SENSOR_NUM_FILTER, SENSOR_FEAT_FILTER_SIZE, DROP_RATE, \n",
        "                        INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Convolutional block with Dropout.\n",
        "\n",
        "    Args:\n",
        "        SENSOR_NUM_FILTER: The number of filters in the Convolutional block.\n",
        "        SENSOR_FEAT_FILTER_SIZE: The filter size of the Convolutional block.\n",
        "        DROP_RATE: The dropout rate for the Convolutional block.\n",
        "        INPUT: The input to the Convolutional block.\n",
        "        NAME: The name of the Convolutional block.\n",
        "\n",
        "    Returns:\n",
        "        x: The output of the Convolutional block.\n",
        "    \"\"\"\n",
        "    x = layers.Conv1D(\n",
        "          SENSOR_NUM_FILTER,\n",
        "          SENSOR_FEAT_FILTER_SIZE,\n",
        "          use_bias=False,\n",
        "          name =  NAME +\"_conv\",\n",
        "          padding=\"same\")(INPUT)\n",
        "    x = layers.ReLU(name =  NAME + \"_relu\")(x)\n",
        "    x = layers.BatchNormalization(name =  NAME + \"_batch_norm\")(x)\n",
        "    x = layers.Dropout(DROP_RATE, name =  NAME + \"_dropout\")(x, training=True)\n",
        "    return x\n",
        "\n",
        "def Convolution(SENSOR_NUM_FILTER, SENSOR_FEAT_FILTER_SIZE, INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Convolutional block.\n",
        "\n",
        "    Args:\n",
        "        SENSOR_NUM_FILTER: The number of filters in the Convolutional block.\n",
        "        SENSOR_FEAT_FILTER_SIZE: The filter size of the Convolutional block.\n",
        "        DROP_RATE: The dropout rate for the Convolutional block.\n",
        "        INPUT: The input to the Convolutional block.\n",
        "        NAME: The name of the Convolutional block.\n",
        "\n",
        "    Returns:\n",
        "        x: The output of the Convolutional block.\n",
        "    \"\"\"\n",
        "    x = layers.Conv1D(\n",
        "          SENSOR_NUM_FILTER,\n",
        "          SENSOR_FEAT_FILTER_SIZE,\n",
        "          use_bias=False,\n",
        "          name =  NAME +\"_conv\",\n",
        "          padding=\"same\")(INPUT)\n",
        "    x = layers.ReLU(name =  NAME + \"_relu\")(x)\n",
        "    x = layers.BatchNormalization(name =  NAME + \"_batch_norm\")(x)\n",
        "    return x\n",
        "\n",
        "def LSTM_LayerNorm(NUM_UNITS, lstm_do, return_seq, INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Recurrent block.\n",
        "\n",
        "    Args:\n",
        "        NUM_UNITS: The number of LSTM units.\n",
        "        lstm_do: The dropout rate for the LSTM block.\n",
        "        return_seq: Boolean whether to return sequence or not.\n",
        "        INPUT: The input to the Recurrent block.\n",
        "        NAME: The name of the Recurrent block.\n",
        "\n",
        "    Returns:\n",
        "        dist: A Normal distribution.\n",
        "    \"\"\"\n",
        "    x = layers.Bidirectional(layers.LSTM(NUM_UNITS, dropout=lstm_do,                          \n",
        "        time_major=False,return_sequences=return_seq, \n",
        "        name =  NAME + \"_lstm\"), name=NAME + \"_bidirectional\")(INPUT, \n",
        "                                                               training=True)\n",
        "    x = layers.LayerNormalization(name =  NAME + \"_layer_norm\")(x)\n",
        "    return x\n",
        "\n",
        "def LSTM(NUM_UNITS, lstm_do, return_seq, INPUT, NAME):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Recurrent block.\n",
        "\n",
        "    Args:\n",
        "        NUM_UNITS: The number of LSTM units.\n",
        "        lstm_do: The dropout rate for the LSTM block.\n",
        "        return_seq: Boolean whether to return sequence or not.\n",
        "        INPUT: The input to the Recurrent block.\n",
        "        NAME: The name of the Recurrent block.\n",
        "\n",
        "    Returns:\n",
        "        dist: A Normal distribution.\n",
        "    \"\"\"\n",
        "    x = layers.Bidirectional(layers.LSTM(NUM_UNITS, dropout=lstm_do,                          \n",
        "        time_major=False,return_sequences=return_seq, \n",
        "        name =  NAME + \"_lstm\"), name=NAME + \"_bidirectional\")(INPUT, \n",
        "                                                               training=True)\n",
        "    return x\n",
        "\n",
        "def Prediction_Dense(NAME, INPUT):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Prediction module.\n",
        "\n",
        "    Args:\n",
        "        INPUT: The input to the prediction module.\n",
        "\n",
        "    Returns:\n",
        "        dist: A Normal distribution.\n",
        "    \"\"\"\n",
        "    #x = layers.Dropout(DROP_RATE, name =  NAME + \"_dropout\")(INPUT, training=True)\n",
        "    params = layers.Dense(2, name=\"Dense_1\")(INPUT)\n",
        "    dist = tfp.layers.DistributionLambda(normal_sp, name=\n",
        "                                         \"Distribution_1\")(params)\n",
        "    return dist\n",
        "\n",
        "def Prediction_Conv_Dense(NAME, INPUT):\n",
        "    \"\"\"\n",
        "    Keras DeepPulse Prediction module.\n",
        "\n",
        "    Args:\n",
        "        INPUT: The input to the prediction module.\n",
        "\n",
        "    Returns:\n",
        "        dist: A Normal distribution.\n",
        "    \"\"\"\n",
        "    x = layers.Conv1D(\n",
        "          1,\n",
        "          16,\n",
        "          use_bias=False,\n",
        "          name =  NAME +\"_conv\",\n",
        "          padding=\"same\")(INPUT)\n",
        "    flatten = layers.Flatten()(x)      \n",
        "    params = layers.Dense(2, name=\"Dense_1\")(flatten)\n",
        "    dist = tfp.layers.DistributionLambda(normal_sp, name=\n",
        "                                         \"Distribution_1\")(params)\n",
        "    return dist\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    \"\"\"\n",
        "    Creates a bandpass Butterworth IIR filter.\n",
        "    Taken from:\n",
        "    scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
        "\n",
        "    Args:\n",
        "        lowcut: The lower cutoff frequency.\n",
        "        highcut: The higher cutoff frequency.\n",
        "        fs: The sample rate.\n",
        "        order: The order of the filter.\n",
        "\n",
        "    Returns:\n",
        "        b: Numerator polynomial of the IIR filter.\n",
        "        a: Denominator polynomial of the IIR filter.\n",
        "    \"\"\"\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = sci_sig.butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    \"\"\"\n",
        "    Applies the bandpass Butterworth IIR filter to the data.\n",
        "    Taken from:\n",
        "    scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
        "\n",
        "    Args:\n",
        "        data: The signal to filter.\n",
        "        lowcut: The lower cutoff frequency.\n",
        "        highcut: The higher cutoff frequency.\n",
        "        fs: The sample rate.\n",
        "        order: The order of the filter.\n",
        "\n",
        "    Returns:\n",
        "        y: The output of the digital filter.\n",
        "    \"\"\"\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = sci_sig.lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def resample_signal(signal_data, curr_fs, new_fs):\n",
        "    \"\"\"\n",
        "    Resample the signal to a different sample rate.\n",
        "\n",
        "    Args:\n",
        "        signal_data: The signal to resample.\n",
        "        curr_fs: The current sampling frequency.\n",
        "        new_fs: The desired sampling frequency.\n",
        "\n",
        "    Returns:\n",
        "        resampled_signal: The output of the resampling\n",
        "    \"\"\"\n",
        "    num_secs_in_signal = len(signal_data) / curr_fs\n",
        "    num_samples_to_resample = round(num_secs_in_signal * new_fs)\n",
        "    resampled_signal = sci_sig.resample(signal_data, num_samples_to_resample)\n",
        "    return resampled_signal\n",
        "\n",
        "def slidingWindow(sequence, winSize, step=1):\n",
        "    \"\"\"\n",
        "    Creates a sliding window generator to iterate through input sequence.\n",
        "    Taken from: https://stackoverflow.com/a/9878083\n",
        "\n",
        "    Args:\n",
        "        sequence: The signal to window.\n",
        "        winSize: The size of sliding window.\n",
        "        step: The size of overlap between windows.\n",
        "\n",
        "    Returns:\n",
        "        sequence[i:i+winSize]: sliding window generator\n",
        "    \"\"\"\n",
        "    num_of_chunks = ((len(sequence) - winSize) / step) + 1\n",
        "    for i in range(0, int(num_of_chunks) * step, step):\n",
        "        yield sequence[i:i + winSize]\n",
        "\n",
        "def preprocess(signal_data, fs, band_pass_low, band_pass_high, band_pass_order,\n",
        "               resample_fs):\n",
        "    \"\"\"\n",
        "    Preprocesses the signals: performs bandpass filtering then resampling.\n",
        "\n",
        "    Args:\n",
        "        signal_data: The raw signal.\n",
        "        fs: The original sample rate of the signal.\n",
        "        band_pass_low: The lower cutoff frequency of the bandpass filter.\n",
        "        band_pass_high: The higher cutoff frequency of the bandpass filter.\n",
        "        band_pass_order: The order of the bandpass filter.\n",
        "        resample_fs: The desired sampling frequency.\n",
        "\n",
        "    Returns:\n",
        "        filtered_signal_resample: The preprocessed signal.\n",
        "    \"\"\"\n",
        "    filtered_signal = butter_bandpass_filter(signal_data, band_pass_low, \n",
        "                                             band_pass_high, fs, \n",
        "                                             order=band_pass_order)\n",
        "    filtered_signal_resample = resample_signal(filtered_signal, fs, resample_fs)\n",
        "    return filtered_signal_resample\n",
        "\n",
        "def flatten_list(t):\n",
        "    \"\"\"\n",
        "    Flattens a 2D list to a 1D list.\n",
        "\n",
        "    Args:\n",
        "        t: The 2D List.\n",
        "\n",
        "    Returns:\n",
        "        a 1D list.\n",
        "    \"\"\"\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "def split_list(alist, wanted_parts=1):\n",
        "    \"\"\"\n",
        "    Splits a list into n lists.\n",
        "    taken from: https://stackoverflow.com/a/752562\n",
        "    Args:\n",
        "        alist: A list to be split.\n",
        "        wanted_parts: The number of splits.\n",
        "\n",
        "    Returns:\n",
        "        list: A N-D list where n = wanted_parts\n",
        "    \"\"\"\n",
        "    length = len(alist)\n",
        "    return [alist[i * length // wanted_parts: (i + 1) * length // \n",
        "                  wanted_parts] for i in range(wanted_parts)]\n",
        "\n",
        "def preprocess_dataset(dataset, band_pass_low, band_pass_high, band_pass_order, \n",
        "                       resample_fs):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset of signals.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset of raw signals.\n",
        "        band_pass_low: The lower cutoff frequency of the bandpass filter.\n",
        "        band_pass_high: The higher cutoff frequency of the bandpass filter.\n",
        "        band_pass_order: The order of the bandpass filter.\n",
        "        resample_fs: The desired sampling frequency.\n",
        "\n",
        "    Returns:\n",
        "        df: A Dataframe containing the preprocessed signals along with \n",
        "            additional information.\n",
        "    \"\"\"\n",
        "    list_of_windows = []\n",
        "    # get sample rates of signals\n",
        "    fs_dataset_PPG = dataset[0][\"PPG_fs\"]\n",
        "    fs_dataset_ACC = dataset[0][\"ACC_fs\"]\n",
        "    # go through each session\n",
        "    for idx, subject in tqdm_notebook(enumerate(range(len(dataset))), \n",
        "                                      desc='Sessions: ', total=len(dataset)):\n",
        "        ppg1_signal_windowed = []\n",
        "        accx_signal_windowed = []\n",
        "        accy_signal_windowed = []\n",
        "        accz_signal_windowed = []\n",
        "        # ---------------PPG------------------------------------------\n",
        "        # preprocess the PPG signal - filter, resample and window\n",
        "        ppg_signal = preprocess(dataset[subject][\"Raw_PPG_1\"], fs_dataset_PPG, \n",
        "                                band_pass_low, band_pass_high, band_pass_order, \n",
        "                                resample_fs)\n",
        "        for i in slidingWindow(ppg_signal, 8 * resample_fs, 2 * resample_fs):\n",
        "            # for each window perform z-norm\n",
        "            # applied on each window to prevent data leakage\n",
        "            i = stats.zscore(i)\n",
        "            ppg1_signal_windowed.append(i)\n",
        "        # ---------------ACC------------------------------------------\n",
        "        # preprocess the ACC signals - filter, resample and window\n",
        "        accx_signal = preprocess(dataset[subject][\"Raw ACC_X\"], fs_dataset_ACC, \n",
        "                                 band_pass_low, band_pass_high, band_pass_order, \n",
        "                                 resample_fs)\n",
        "        for i in slidingWindow(accx_signal, 8 * resample_fs, 2 * resample_fs):\n",
        "            i = stats.zscore(i)\n",
        "            accx_signal_windowed.append(i)\n",
        "        accy_signal = preprocess(dataset[subject][\"Raw ACC_Y\"], fs_dataset_ACC,\n",
        "                                 band_pass_low,band_pass_high, band_pass_order, \n",
        "                                 resample_fs)\n",
        "        for i in slidingWindow(accy_signal, 8 * resample_fs, 2 * resample_fs):\n",
        "            i = stats.zscore(i)\n",
        "            accy_signal_windowed.append(i)\n",
        "        accz_signal = preprocess(dataset[subject][\"Raw ACC_Z\"], fs_dataset_ACC, \n",
        "                                 band_pass_low,band_pass_high, band_pass_order,\n",
        "                                 resample_fs)\n",
        "        for i in slidingWindow(accz_signal, 8 * resample_fs, 2 * resample_fs):\n",
        "            i = stats.zscore(i)\n",
        "            accz_signal_windowed.append(i)\n",
        "        # convert the lists to np.arrays\n",
        "        ppg1_signal_windowed = np.array(ppg1_signal_windowed)\n",
        "        accx_signal_windowed = np.array(accx_signal_windowed)\n",
        "        accy_signal_windowed = np.array(accy_signal_windowed)\n",
        "        accz_signal_windowed = np.array(accz_signal_windowed)\n",
        "        # stack the ACC arrays into a tensor with shape\n",
        "        # (Num of windows, length of windows, 3)\n",
        "        acc_signals_stacked = np.stack((accx_signal_windowed,\n",
        "                                        accy_signal_windowed, \n",
        "                                        accz_signal_windowed), axis=2)\n",
        "        # Expand the dimension of the PPG tensor to have shape\n",
        "        # (Num of windows, length of windows, 1)\n",
        "        ppg1_signal_windowed = np.expand_dims(ppg1_signal_windowed, axis=2)\n",
        "        # go thru each window and make a dict containing all the requried info.\n",
        "        for window in range(len(ppg1_signal_windowed)):\n",
        "            dict_entry = {\n",
        "                \"Signal_ID\": idx,\n",
        "                \"Subject\": dataset[subject][\"Subject\"],\n",
        "                \"Window_ID\": window,\n",
        "                \"PPG\": ppg1_signal_windowed[window],\n",
        "                \"ACC\": acc_signals_stacked[window],\n",
        "                \"SNR\": dataset[subject][\"SNR\"][window],\n",
        "                \"Activity\": dataset[subject][\"Protocol\"][window],\n",
        "                \"Truth\": dataset[subject][\"truth_values\"][window]\n",
        "            }\n",
        "            list_of_windows.append(dict_entry)\n",
        "    # turn the list of dicts to a DF.\n",
        "    df = pd.DataFrame(list_of_windows)\n",
        "    return df\n",
        "\n",
        "def get_test_results(model, df_test, fold, model_name, num_samples):\n",
        "    \"\"\"\n",
        "    Get the results from the unseen session data.\n",
        "\n",
        "    Args:\n",
        "        model: The deep learning model.\n",
        "        df_test: The dataframe of the unseen session data.\n",
        "        fold: The fold of the LOSO CV.\n",
        "        model_name: The name of the model.\n",
        "        num_samples: The number of samples from the predictive distribution.\n",
        "\n",
        "    Returns:\n",
        "        df_calibration: A Dataframe containing the calibration metrics.\n",
        "        df_uncertainty: A Dataframe containing the uncertainty metrics.\n",
        "\n",
        "    \"\"\"\n",
        "    result_dict = []\n",
        "    # create instance of model with the last layer of the model removed\n",
        "    model_deeppulse = keras.Model(inputs=model.input, outputs=\n",
        "                                  model.layers[-2].output)\n",
        "    # for each window\n",
        "    for index, row in tqdm_notebook(df_test.iterrows(), desc='Windows', \n",
        "                                    total=df_test.shape[0]):\n",
        "        # repeat num_samples times to get samples from predictive distribution\n",
        "        for d in range(num_samples):\n",
        "            # get the model output\n",
        "            x = model_deeppulse.predict([np.expand_dims(row[\"PPG\"], axis=0), \n",
        "                                         np.expand_dims(row[\"ACC\"], axis=0)],\n",
        "                                        verbose=0)\n",
        "            #make the model output a 1D list of floats\n",
        "            output = [float(i) for i in x[0]]\n",
        "            # convert model output to distribution\n",
        "            distri = normal_sp_predict(output)\n",
        "            # put info into dict to analyse\n",
        "            dicts = {'Signal_ID': row[\"Signal_ID\"],\n",
        "                     'Subject': row[\"Subject\"],\n",
        "                     'Window': row[\"Window_ID\"],\n",
        "                     'SNR': row[\"SNR\"],\n",
        "                     'Activity': row[\"Activity\"],\n",
        "                     'Truth': row[\"Truth\"],\n",
        "                     'Model_ID': fold,\n",
        "                     'Ensemble_ID': d,\n",
        "                     'Mean_Value': distri.mean().numpy(),\n",
        "                     'STD_Value': distri.stddev().numpy()}\n",
        "            result_dict.append(dicts)\n",
        "    # create DF from list of dicts\n",
        "    df_calibration = pd.DataFrame(result_dict)\n",
        "    # save df as csv\n",
        "    file_name = model_name[:-3] + str(\"results_cali.csv\")\n",
        "    df_calibration.to_csv(file_name, index=False)\n",
        "    # compute mean predicted value for each window\n",
        "    groupby_mean_predict = df_calibration.groupby([\"Signal_ID\", \"Subject\", \n",
        "                                                   \"Window\", \"Activity\",\n",
        "                                                   \"SNR\", \"Truth\"], \n",
        "                                            as_index=False)[\"Mean_Value\"].mean()\n",
        "    groupby_mean_predict[\"Absolute Error\"] = abs(groupby_mean_predict[\"Truth\"]\n",
        "                                          - groupby_mean_predict[\"Mean_Value\"])\n",
        "    # compute Epistemic uncertainty\n",
        "    groupby_epistemic = df_calibration.groupby([\"Signal_ID\", \"Subject\", \n",
        "                                                \"Window\"])['Mean_Value'].var() \\\n",
        "        .reset_index(name=\"Epistemic\")\n",
        "    epistemic_column = groupby_epistemic[\"Epistemic\"]\n",
        "    df_uncertainty = pd.concat([groupby_mean_predict, epistemic_column], axis=1)\n",
        "    # compute Aleotoric uncertainty\n",
        "    groupby_aleotoric = df_calibration.groupby([\"Signal_ID\", \"Subject\", \n",
        "                                              \"Window\"])['STD_Value'].median() \\\n",
        "        .reset_index(name=\"Aleotoric\")\n",
        "    aleotoric_column = groupby_aleotoric[\"Aleotoric\"]\n",
        "    df_uncertainty = pd.concat([df_uncertainty, aleotoric_column], axis=1)\n",
        "    # compute predictive uncertainty\n",
        "    df_uncertainty[\"Predictive\"] = df_uncertainty[\"Epistemic\"] + df_uncertainty[\"Aleotoric\"]\n",
        "    df_uncertainty.rename(columns={'Mean_Value': 'Predictive_Mean'}, inplace=True)\n",
        "    # save df as csv\n",
        "    file_name = model_name[:-3] + str(\"results_uncert.csv\")\n",
        "    df_uncertainty.to_csv(file_name, index=False)\n",
        "    return df_calibration, df_uncertainty\n",
        "\n",
        "def get_percentage_overlap_of_fold_sets(list_of_windows):\n",
        "    \"\"\"\n",
        "    Get the percentage overlap of the set of data used for each fold, i.e. the training set/ the val set.\n",
        "    \"percentage\" take from: https://stackoverflow.com/a/29929179\n",
        "\n",
        "    Args:\n",
        "        list_of_windows: A list of lists containing all the windows in the set of data used for each fold.\n",
        "\n",
        "    Returns:\n",
        "        data: A Dataframe containing the percentage overlap for each fold.\n",
        "        matrix: The upper triangle of the Dataframe.\n",
        "\n",
        "    \"\"\"\n",
        "    dict_of_percentages = []\n",
        "    # get all possible combinations of fold into a list of tuples\n",
        "    x = list(itertools.product(range(len(list_of_windows)), repeat=2))\n",
        "    # for each tuple\n",
        "    for i in range(len(x)):\n",
        "        # split the lists into a list for each fold in the tuple\n",
        "        first_list = x[i][0]\n",
        "        second_list = x[i][1]\n",
        "        # compute percentage overlap between the two lists\n",
        "        percentage = len(set(list_of_windows[first_list]) & set(list_of_windows[second_list])) / \\\n",
        "                     float(len(set(list_of_windows[first_list]) | set(list_of_windows[second_list]))) * 100\n",
        "        # store result in dict\n",
        "        result = {\n",
        "            \"Fold X\": first_list,\n",
        "            \"Fold Y\": second_list,\n",
        "            \"percentage\": percentage\n",
        "        }\n",
        "        dict_of_percentages.append(result)\n",
        "    # store in df\n",
        "    df = pd.DataFrame(dict_of_percentages)\n",
        "    data = df.pivot(\"Fold X\", \"Fold Y\", \"percentage\")\n",
        "    # The upper triangle of the data\n",
        "    matrix = np.triu(data)\n",
        "    return data, matrix\n",
        "\n",
        "def get_dataset_splits_loso(DATASET, stratified, test_sze, test_subject):\n",
        "    \"\"\"\n",
        "    Get the train, validation and test sets for the current fold\n",
        "\n",
        "    Args:\n",
        "        DATASET: The dataframe of all windows.\n",
        "        stratified: Whether the train/val sets are stratified on activity type.\n",
        "        test_sze: The percentage of the data used for validation set.\n",
        "        test_subject: The session ID for the test set.\n",
        "\n",
        "    Returns:\n",
        "        df_train: A Dataframe containing the training data.\n",
        "        df_val: A Dataframe containing the validation data.\n",
        "        df_test: A Dataframe containing the test data.\n",
        "\n",
        "    \"\"\"\n",
        "    # get the windows that are associated with the test subject\n",
        "    df_test = shuffle(DATASET.loc[DATASET['Signal_ID'] == test_subject])\n",
        "    # get the train + val sets either stratified or not\n",
        "    if stratified:\n",
        "        df_train, df_val = train_test_split(DATASET, test_size=test_sze, shuffle=True, stratify=DATASET['Activity'])\n",
        "    else:\n",
        "        df_train, df_val = train_test_split(DATASET, test_size=test_sze, shuffle=True)\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "def display_training_performance(df_result, history, eval_history, fold, path, display, publication):\n",
        "    \"\"\"\n",
        "    Display the training performance of the model on the fold of data.\n",
        "\n",
        "    Args:\n",
        "        df_result: The dataframe of all windows with labels of the set name (i.e. train set).\n",
        "        history: The training history dict.\n",
        "        eval_history: The testing history dict.\n",
        "        fold: The current fold of the LOSO CV.\n",
        "        path: The path of the experiment.\n",
        "        display: Whether to display inline during training.\n",
        "        publication: Whether to export as svg or png format.\n",
        "    \"\"\"\n",
        "  \n",
        "    if publication:\n",
        "        fig_format = 'svg'\n",
        "    else:\n",
        "        fig_format = 'png'\n",
        "    fig = plt.figure(figsize=(40, 25))\n",
        "    my_suptitle = fig.suptitle('Training for Fold ' + str(fold), y=1.05)\n",
        "    gs = GridSpec(nrows=3, ncols=5)\n",
        "    # Bar chart of the size differences of the data splits\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    ax0.title.set_text(\"Size of Dataset Splits\")\n",
        "    sns.countplot(ax=ax0, x=\"Dataset\", data=df_result)\n",
        "    ax0.set_xticklabels(ax0.get_xticklabels(), rotation=0)\n",
        "\n",
        "    # Graph showing the distribution of the truth values for each split of data\n",
        "    ax1 = fig.add_subplot(gs[0, 1])\n",
        "    ax1.title.set_text(\"Distribution of Truth Values\")\n",
        "    sns.kdeplot(x='Truth', fill=False, data=df_result, hue=\"Dataset\", ax=ax1)\n",
        "\n",
        "    # Graph showing the distribution of the Signal to Noise Ratios for each split of data\n",
        "    ax2 = fig.add_subplot(gs[0, 2])\n",
        "    ax2.title.set_text(\"Distribution of Estimated Signal to Noise Ratio\")\n",
        "    sns.kdeplot(x='SNR', fill=False, data=df_result, hue=\"Dataset\", ax=ax2)\n",
        "\n",
        "    # Graph showing the distribution of the Activity Types for each split of data\n",
        "    ax3 = fig.add_subplot(gs[0, 3])\n",
        "    ax3.title.set_text(\"Distribution of Activity Types\")\n",
        "    sns.countplot(x=\"Activity\", hue=\"Dataset\", data=df_result, ax=ax3)\n",
        "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=75)\n",
        "\n",
        "    # Graph showing the Learning Rate Whilst Training\n",
        "    ax4 = fig.add_subplot(gs[0, 4])\n",
        "    ax4.title.set_text(\"Learning Rate Whilst Training\")\n",
        "    ax4.plot(history.history['lr'])\n",
        "    ax4.set(xlabel=\"Epoch\", ylabel=\"Learning Rate\")\n",
        "\n",
        "    # Graph showing the MAE Learning Curve\n",
        "    ax5 = fig.add_subplot(gs[1, 0:4])\n",
        "    ax5.title.set_text(\"Learning Curve (MAE)\")\n",
        "    ax5.plot(history.history['mae'], label=\"Train\")\n",
        "    ax5.plot(history.history['val_mae'], label=\"Validation\")\n",
        "    ax5.plot([eval_history[1]] * len(history.history['mae']), label=\"Test: \" + str(round(eval_history[1], 3)))\n",
        "    ax5.axvline(x=np.argmin(history.history['val_loss']), color='k', linestyle='--',\n",
        "                label=\"Saved Model @ Epoch: \" + str(np.argmin(history.history['val_loss'])))\n",
        "    ax5.set(xlabel=\"Epoch\", ylabel=\"MAE\")\n",
        "    ax5.legend(loc='upper right')\n",
        "\n",
        "    # Graph showing the NLL Learning Curve\n",
        "    ax6 = fig.add_subplot(gs[2, 0:4])\n",
        "    ax6.title.set_text(\"Learning Curve (NLL)\")\n",
        "    ax6.plot(history.history['loss'], label=\"Train\")\n",
        "    ax6.plot(history.history['val_loss'], label=\"Validation\")\n",
        "    ax6.plot([eval_history[0]] * len(history.history['loss']), label=\"Test: \" + str(round(eval_history[0], 3)))\n",
        "    ax6.axvline(x=np.argmin(history.history['val_loss']), color='k', linestyle='--',\n",
        "                label=\"Saved Model @ Epoch: \" + str(np.argmin(history.history['val_loss'])))\n",
        "    ax6.set_yscale('log')\n",
        "    ax6.set(xlabel=\"Epoch\", ylabel=\"NLL\")\n",
        "    ax6.legend(loc='upper right')\n",
        "\n",
        "    # Bar chart showing the performance of MAE @ model checkpoint\n",
        "    ax7 = fig.add_subplot(gs[1, 4:5])\n",
        "    ax7.title.set_text(\"Performance of Learning (MAE)\")\n",
        "    name = [\"Train\", \"Validate\", \"Test\"]\n",
        "    lii = [history.history['mae'][np.argmin(history.history['val_loss'])],\n",
        "           history.history['val_mae'][np.argmin(history.history['val_loss'])], round(eval_history[1], 3)]\n",
        "    sns.barplot(ax=ax7, x=name, y=lii)\n",
        "    ax7.set(xlabel=\"Performance @ \" + str(np.argmin(history.history['val_loss'])), ylabel=\"MAE\")\n",
        "    \n",
        "    # Bar chart showing the performance of NLL @ model checkpoint\n",
        "    ax8 = fig.add_subplot(gs[2, 4:5])\n",
        "    ax8.title.set_text(\"Performance of Learning (NLL)\")\n",
        "    name = [\"Train\", \"Validate\", \"Test\"]\n",
        "    lii = [history.history['loss'][np.argmin(history.history['val_loss'])],\n",
        "           history.history['val_loss'][np.argmin(history.history['val_loss'])], round(eval_history[0], 3)]\n",
        "    sns.barplot(ax=ax8, x=name, y=lii)\n",
        "    ax8.set(xlabel=\"Performance @ Epoch\" + str(np.argmin(history.history['val_loss'])), ylabel=\"NLL\")\n",
        "    \n",
        "    gs.tight_layout(fig)\n",
        "    # either save file as svg or png\n",
        "    fig_name = path + '/fold' + str(fold) + 'training.' + fig_format\n",
        "    fig.savefig(fig_name, format=fig_format, dpi=fig.dpi,\n",
        "                bbox_inches='tight', bbox_extra_artists=[my_suptitle])\n",
        "    # display graph during execution or not\n",
        "    if display:\n",
        "        plt.show()\n",
        "        time.sleep(10)\n",
        "    plt.close(fig)\n",
        "    output.clear()\n",
        "\n",
        "def display_calibration_performance(calibration_df, uncert_df, fold, path, display, publication):\n",
        "    \"\"\"\n",
        "    Display the uncertainty & uncertainty calibration performance of the model on the fold of data.\n",
        "\n",
        "    Args:\n",
        "        calibration_df: A Dataframe containing the calibration metrics.\n",
        "        uncert_df: A Dataframe containing the uncertainty metrics.\n",
        "        fold: The current fold of the LOSO CV.\n",
        "        path: The path of the experiment.\n",
        "        display: Whether to display inline during training.\n",
        "        publication: Whether to export as svg or png format.\n",
        "    \"\"\"\n",
        "    # convert df columns to np arrays\n",
        "    y_true = calibration_df['Truth'].to_numpy()\n",
        "    # x = calibration_df['Window'].to_numpy()\n",
        "    y_pred = calibration_df['Mean_Value'].to_numpy()\n",
        "    y_std = calibration_df['STD_Value'].to_numpy()\n",
        "    # Set random seed\n",
        "    np.random.seed(11)\n",
        "    fig = plt.figure(figsize=(40, 40))\n",
        "    my_suptitle = fig.suptitle('Calibration for Fold ' + str(fold), y=1.05)\n",
        "    gs = GridSpec(nrows=3, ncols=3)\n",
        "    # graph showing the predicted values against the truth along with activity type, SNR and both uncertainty metrics\n",
        "    ax010 = fig.add_subplot(gs[0, 0:3])\n",
        "    # get the start indexes of each activity for the session\n",
        "    start_indexes, activity = repeating_values(uncert_df[\"Activity\"])\n",
        "    colours = ['#42d4f4', '#f032e6', '#fabed4', '#469990', '#dcbeff', '#9A6324', '#fffac8',\n",
        "               '#800000', '#aaffc3', '#000075', '#a9a9a9']\n",
        "    # get upper and lower bands of the uncertainty terms for each window\n",
        "    activities = uncert_df[\"Activity\"].unique()\n",
        "    pred = list(uncert_df[\"Predictive_Mean\"])\n",
        "    aleo = list(uncert_df[\"Aleotoric\"])\n",
        "    epi = list(uncert_df[\"Epistemic\"])\n",
        "    lower_aleo = np.array(pred) - np.array(aleo)\n",
        "    upper_aleo = np.array(pred) + np.array(aleo)\n",
        "    lower_epi = np.array(pred) - np.array(epi)\n",
        "    upper_epi = np.array(pred) + np.array(epi)\n",
        "\n",
        "    ax010.plot(range(len(uncert_df[\"Truth\"])), uncert_df[\"Truth\"])\n",
        "    x_ax = np.arange(0, len(uncert_df[\"Predictive_Mean\"]), 1)\n",
        "    # plot Predicted values\n",
        "    ax010.plot(x_ax, uncert_df[\"Predictive_Mean\"], '-', label=\"Predicted\", color='#e6194B')\n",
        "    # plot truth values\n",
        "    ax010.plot(x_ax, uncert_df[\"Truth\"], '-', label=\"Truth\", color='#000000')\n",
        "    ax020 = ax010.twinx()\n",
        "    # plot SNR values\n",
        "    ax020.plot(x_ax, uncert_df[\"SNR\"], '-', label=\"Est. SNR\", color='#3cb44b')\n",
        "    # plot activity type\n",
        "    for idx, act in enumerate(activity):\n",
        "        if idx + 1 == len(activity):\n",
        "            ax010.axvspan(start_indexes[idx], len(uncert_df[\"Truth\"]),\n",
        "                          color=colours[int(np.where(act == activities)[0])],\n",
        "                          alpha=0.1, label=activities[int(np.where(act == activities)[0])])\n",
        "        else:\n",
        "            ax010.axvspan(start_indexes[idx], start_indexes[idx + 1],\n",
        "                          color=colours[int(np.where(act == activities)[0])],\n",
        "                          alpha=0.1, label=activities[int(np.where(act == activities)[0])])\n",
        "    # plot Aleotoric Uncertainty\n",
        "    ax010.fill_between(x_ax, lower_aleo, upper_aleo,\n",
        "                       color='#4363d8', alpha=0.5, label=\"Aleotoric Uncertainty\")\n",
        "    # plot Epistemic Uncertainty\n",
        "    ax010.fill_between(x_ax, lower_epi, upper_epi,\n",
        "                       color='#f58231', alpha=0.5, label=\"Epistemic Uncertainty\")\n",
        "\n",
        "    lines, labels = ax010.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax020.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, lines))\n",
        "    by_label.update(zip(labels2, lines2))\n",
        "    ax010.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
        "                 ncol=4, mode=\"expand\", borderaxespad=0.)\n",
        "    ax010.set_xlabel('Windows')\n",
        "    ax010.set_ylabel('Beats Per Minute')\n",
        "    ax020.set_ylabel('Signal to Noise Ratio (dB)')\n",
        "    # Plot calibration of predictive Uncertainty\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    uct.plot_calibration(y_pred, y_std, y_true, ax=ax3)\n",
        "    # Plot adversarial group calibration of predictive Uncertainty\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    uct.plot_adversarial_group_calibration(y_pred, y_std, y_true, ax=ax4)\n",
        "    # Plot sharpness of predictive Uncertainty\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    uct.plot_sharpness(y_std, ax=ax5)\n",
        "    # Plot boxplots of Epistemic Uncertainty for each activity type\n",
        "    ax6 = fig.add_subplot(gs[2, 0])\n",
        "    sns.boxplot(x='Activity', y='Epistemic', data=uncert_df, showfliers=False, orient='v', ax=ax6)\n",
        "    ax65 = ax6.twinx()\n",
        "    sns.countplot(x='Activity', data=uncert_df, alpha=0.35, ax=ax65)\n",
        "    # Plot boxplots of Aleotoric Uncertainty for each activity type\n",
        "    ax7 = fig.add_subplot(gs[2, 1])\n",
        "    sns.boxplot(x='Activity', y='Aleotoric', data=uncert_df, showfliers=False, orient='v', ax=ax7)\n",
        "    # plot relationship between Epistemic & Aleotoric Uncertainty\n",
        "    ax8 = fig.add_subplot(gs[2, 2])\n",
        "    sns.scatterplot(x='Aleotoric', y='Epistemic', data=uncert_df, ax=ax8)\n",
        "    gs.tight_layout(fig)\n",
        "    # either save file as svg or png\n",
        "    if publication:\n",
        "        fig_format = 'svg'\n",
        "    else:\n",
        "        fig_format = 'png'\n",
        " \n",
        "    fig_plot_name = 'calibration.'    \n",
        "    fig_name = path + '/fold' + str(fold) + fig_plot_name + fig_format\n",
        "    fig.savefig(fig_name, format=fig_format, dpi=fig.dpi, bbox_inches='tight',\n",
        "                bbox_extra_artists=[my_suptitle])\n",
        "    # display graph during execution or not\n",
        "    if display:\n",
        "        plt.show()\n",
        "        time.sleep(10)\n",
        "    plt.close(fig)\n",
        "    output.clear()\n",
        "\n",
        "def display_overall_performance(path, unique_check_train, unique_check_val, \n",
        "                                unique_check_test, display, publication):\n",
        "    \"\"\"\n",
        "    Display the overall performance of the model on all the fold of data.\n",
        "    unique_check_train, unique_check_val, unique_check_test,\n",
        "    Args:\n",
        "        path: The path of the experiment.\n",
        "        unique_check_train: list of list containing all windows used for training for each fold.\n",
        "        unique_check_val: list of list containing all windows used for validation for each fold.\n",
        "        unique_check_test: list of list containing all windows used for testing for each fold.\n",
        "        display: Whether to display inline during training.\n",
        "        publication: Whether to export as svg or png format.\n",
        "    \"\"\"\n",
        "    all_uncert_df, all_calibration_df, df_session_specific, df_overall = get_overall_stats(path)\n",
        "    # get calibration vals to numpy arrays\n",
        "    y_true = all_calibration_df['Truth'].to_numpy()\n",
        "    y_pred = all_calibration_df['Mean_Value'].to_numpy()\n",
        "    y_std = all_calibration_df['STD_Value'].to_numpy()\n",
        "    # Set random seed\n",
        "    np.random.seed(11)\n",
        "    fig = plt.figure(figsize=(50, 50))\n",
        "\n",
        "    my_suptitle = fig.suptitle('Overall Results', y=1.05)\n",
        "    gs = GridSpec(nrows=6, ncols=4)\n",
        "    # Plot MAE for each session \n",
        "    ax001 = fig.add_subplot(gs[0, 0])\n",
        "    all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].mean().plot.bar(ax=ax001)\n",
        "    ax001.axhline(np.mean(all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].mean()), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].mean()),2)) + ' BPM')\n",
        "    ax001.set_ylabel('Mean Absolute Error')\n",
        "    ax001.legend(loc='best')\n",
        "    ax001.set_title(\"Mean Absolute Error By Session\")\n",
        "    # Plot MAX AE for each session\n",
        "    ax002 = fig.add_subplot(gs[0, 1])\n",
        "    all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].max().plot.bar(ax=ax002)\n",
        "    ax002.axhline(np.mean(all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].max()), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(all_uncert_df.groupby(\"Signal_ID\")['Absolute Error'].max()),2)) + ' BPM')\n",
        "    ax002.set_ylabel('Max Absolute Error')\n",
        "    ax002.legend(loc='best')\n",
        "    ax002.set_title(\"Max Absolute Error By Session\")\n",
        "    # Plot MAE for each Activity TYPE\n",
        "    ax003 = fig.add_subplot(gs[0, 2])\n",
        "    all_uncert_df.groupby(\"Activity\")['Absolute Error'].mean().plot.bar(ax=ax003)\n",
        "    ax003.axhline(np.mean(all_uncert_df.groupby(\"Activity\")['Absolute Error'].mean()), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(all_uncert_df.groupby(\"Activity\")['Absolute Error'].mean()),2)) + ' BPM')\n",
        "    ax003.set_ylabel('Mean Absolute Error')\n",
        "    ax003.legend(loc='best')\n",
        "    ax003.set_title(\"Mean Absolute Error By Activity\")\n",
        "    # Plot MAE for each Activity TYPE\n",
        "    ax004 = fig.add_subplot(gs[0, 3])\n",
        "    all_uncert_df.groupby(\"Fold\", sort=False)['Absolute Error'].mean().plot.bar(ax=ax004)\n",
        "    ax004.axhline(np.mean(all_uncert_df.groupby(\"Fold\")['Absolute Error'].mean()), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(all_uncert_df.groupby(\"Fold\")['Absolute Error'].mean()),2)) + ' BPM')\n",
        "    ax004.set_ylabel('Mean Absolute Error')\n",
        "    ax004.legend(loc='best')\n",
        "    ax004.set_title(\"Mean Absolute Error By Fold\")\n",
        "    \n",
        "    # Plot heatmap of overlap between folds for train sets\n",
        "    ax005 = fig.add_subplot(gs[1, 0])\n",
        "    data, matrix = get_percentage_overlap_of_fold_sets(unique_check_train)\n",
        "    sns.heatmap(data, mask=matrix, cmap=\"RdYlBu\", ax=ax005)\n",
        "    ax005.set_title(\"% Overlap Between Training Sets for Each Fold\")\n",
        "    # Plot heatmap of overlap between folds for val sets\n",
        "    ax006 = fig.add_subplot(gs[1, 1])\n",
        "    data, matrix = get_percentage_overlap_of_fold_sets(unique_check_val)\n",
        "    sns.heatmap(data, mask=matrix, cmap=\"RdYlBu\", ax=ax006)\n",
        "    ax006.set_title(\"% Overlap Between Validation Sets for Each Fold\")\n",
        "    # Plot heatmap of overlap between folds for test sets\n",
        "    ax007 = fig.add_subplot(gs[1, 2])\n",
        "    data, matrix = get_percentage_overlap_of_fold_sets(unique_check_test)\n",
        "    sns.heatmap(data, mask=matrix, cmap=\"RdYlBu\", ax=ax007)\n",
        "    ax007.set_title(\"% Overlap Between Test Sets for Each Fold\")\n",
        "    # plot calibration plot\n",
        "    ax007 = fig.add_subplot(gs[1, 3])\n",
        "    avg_len_train_val_test = [average_len(unique_check_train), \n",
        "                              average_len(unique_check_val), \n",
        "                             average_len(unique_check_test)]\n",
        "    avg_len_names = [\"Train\", \"Validation\", \"Test\"]\n",
        "    ax007.bar(avg_len_names,avg_len_train_val_test)\n",
        "    ax007.set_ylabel('Number of Samples')\n",
        "    ax007.set_title('Average Size of Each Data Split')\n",
        "    \n",
        "    ax008 = fig.add_subplot(gs[2, 0])\n",
        "    sns.scatterplot(x='Truth', y='Absolute Error', data=all_uncert_df, ax=ax008)\n",
        "    ax008.set_title('Relationship Between Truth and Absolute Error')\n",
        "    # Plot relationship between SNR and AE\n",
        "    ax009 = fig.add_subplot(gs[2, 1])\n",
        "    sns.scatterplot(x='SNR', y='Absolute Error', data=all_uncert_df, ax=ax009)\n",
        "    ax009.set_title('Relationship Between Est. SNR and Absolute Error')\n",
        "    # Plot relationship between SNR and AE\n",
        "    ax010 = fig.add_subplot(gs[2, 2])\n",
        "    sns.boxplot(x='Activity', y='Absolute Error', data=all_uncert_df, showfliers=False, orient='v', ax=ax010)\n",
        "    ax010.set_title('Relationship Between Activity and Absolute Error')\n",
        "    # Plot bia/var\n",
        "    ax011 = fig.add_subplot(gs[2, 3])\n",
        "    sns.barplot(x='Fold', y='Session_Bias', data=df_session_specific, ax=ax011)\n",
        "    ax011.axhline(np.mean(df_session_specific['Session_Bias']), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(df_session_specific['Session_Bias']),2)) + ' BPM')\n",
        "    ax011.set_xlabel('Fold')\n",
        "    ax011.set_ylabel('Bias')\n",
        "    ax011.set_title('Bias per Fold')\n",
        "    ax011.legend(loc='best')\n",
        "\n",
        "    ax012 = fig.add_subplot(gs[3, 0])\n",
        "    uct.plot_calibration(y_pred, y_std, y_true, ax=ax012)\n",
        "    # plot adversarial group calibration plot\n",
        "    ax013 = fig.add_subplot(gs[3, 1])\n",
        "    uct.plot_adversarial_group_calibration(y_pred, y_std, y_true, ax=ax013)\n",
        "    ax014 = fig.add_subplot(gs[3, 2])\n",
        "    sns.barplot(x='Fold', y='Session_Miscal_Area', data=df_session_specific, ax=ax014)\n",
        "    ax014.axhline(np.mean(df_session_specific['Session_Miscal_Area']), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(df_session_specific['Session_Miscal_Area']),2)) + ' BPM')\n",
        "    ax014.set_xlabel('Fold')\n",
        "    ax014.set_ylabel('Miscalibration Area')\n",
        "    ax014.set_title('Miscalibration Area per Fold')\n",
        "    ax014.legend(loc='best')\n",
        "    \n",
        "    ax015 = fig.add_subplot(gs[3, 3])\n",
        "    sns.barplot(x='Fold', y='Session_Var', data=df_session_specific, ax=ax015)\n",
        "    ax015.axhline(np.mean(df_session_specific['Session_Var']), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(df_session_specific['Session_Var']),2)) + ' BPM')\n",
        "    ax015.set_xlabel('Fold')\n",
        "    ax015.set_ylabel('Variance')\n",
        "    ax015.set_title('Variance per Fold')\n",
        "    ax015.legend(loc='best')\n",
        "    # plot sharpness plot\n",
        "    ax016 = fig.add_subplot(gs[4, 0])\n",
        "    uct.plot_sharpness(y_std, ax=ax016)\n",
        "    ax017 = fig.add_subplot(gs[4, 1])\n",
        "    sns.barplot(x='Fold', y='Session_Sharpness', data=df_session_specific, ax=ax017)\n",
        "    ax017.axhline(np.mean(df_session_specific['Session_Sharpness']), ls='--',\n",
        "                  label=\"Average @\" + str(\n",
        "                      round(np.mean(df_session_specific['Session_Sharpness']),2)) + ' BPM')\n",
        "    ax017.set_xlabel('Fold')\n",
        "    ax017.set_ylabel('Sharpness')\n",
        "    ax017.set_title('Sharpness per Fold')\n",
        "    # plot boxplot of activity type and epistemic uncert\n",
        "    ax018 = fig.add_subplot(gs[4, 2])\n",
        "    sns.boxplot(x='Activity', y='Epistemic', data=all_uncert_df, showfliers=False, orient='v', ax=ax018)\n",
        "    ax0185 = ax018.twinx()\n",
        "    sns.countplot(x='Activity', data=all_uncert_df, alpha=0.5, ax=ax0185)\n",
        "    ax0185.tick_params('x', labelrotation=30)\n",
        "    ax018.set_title('Relationship Between Activity and Epistemic Uncertainty')\n",
        "    # plot boxplot of activity type and Aleotoric uncert\n",
        "    ax019 = fig.add_subplot(gs[4, 3])\n",
        "    sns.boxplot(x='Activity', y='Aleotoric', data=all_uncert_df, showfliers=False, orient='v', ax=ax019)\n",
        "    ax019.tick_params('x', labelrotation=30)\n",
        "    ax019.set_title('Relationship Between Activity and Aleotoric Uncertainty')\n",
        "    # plot relationship between Aleotoric & Epistemic uncert\n",
        "    ax020 = fig.add_subplot(gs[5, 0])\n",
        "    sns.scatterplot(x='SNR', y='Aleotoric', data=all_uncert_df, ax=ax020)\n",
        "    ax020.set_title('Relationship Between Est. SNR and Aleotoric Uncertainty')\n",
        "    ax021 = fig.add_subplot(gs[5, 1])\n",
        "    sns.scatterplot(x='SNR', y='Epistemic', data=all_uncert_df, ax=ax021)\n",
        "    ax021.set_title('Relationship Between Est. SNR and Epistemic Uncertainty')\n",
        "    ax021.set_yscale('log')\n",
        "    ax022 = fig.add_subplot(gs[5, 2])\n",
        "    ax022.set_yscale('log')\n",
        "    sns.scatterplot(x='Aleotoric', y='Epistemic', data=all_uncert_df, ax=ax022)\n",
        "    ax022.set_title('Relationship Between Aleotoric and Epistemic Uncertainty')\n",
        "    \n",
        "    gs.tight_layout(fig)\n",
        "\n",
        "    # either save file as svg or png\n",
        "    if publication:\n",
        "        fig_format = 'svg'\n",
        "    else:\n",
        "        fig_format = 'png'\n",
        "    fig_name = path + '/overall.' + fig_format\n",
        "    fig.savefig(fig_name, format=fig_format, dpi=fig.dpi, bbox_inches='tight',\n",
        "                bbox_extra_artists=[my_suptitle])\n",
        "    # display graph during execution or not\n",
        "    if display:\n",
        "        plt.show()\n",
        "        time.sleep(10)\n",
        "    plt.close(fig)\n",
        "    output.clear()\n",
        "\n",
        "def get_overall_stats(path):\n",
        "    \"\"\"\n",
        "    Gets overall stats of the performance of the model\n",
        "\n",
        "    Args:\n",
        "        path: The path of the experiment.\n",
        "\n",
        "    Returns:\n",
        "        all_uncert_df: Dataframe containing the uncertainty metrics.\n",
        "        all_calibration_df: Dataframe containing the calibration metrics.\n",
        "        df_session_specific: Dataframe containing session/fold specific metrics.\n",
        "        df_overall: Dataframe containing overall metrics to compare with other models.\n",
        "    \"\"\"\n",
        "    # init lists to store metrics\n",
        "    calibration_results = []\n",
        "    fold = []\n",
        "    sess_mae = []\n",
        "    sess_max_ae = []\n",
        "    sess_ma = []\n",
        "    sess_sharp = []\n",
        "    sess_expected_loss = []\n",
        "    sess_bias = []\n",
        "    sess_var = []\n",
        "    cali_filename = '*results_cali.csv'\n",
        "    uncert_filename = '*results_uncert.csv'\n",
        "    sess_spec_file_name = \"results_fold_specific.csv\"\n",
        "    overall_file_name = \"results_overall.csv\"\n",
        "    #get all the fold results into 1 df\n",
        "    for filename in os.listdir(path):\n",
        "        if fnmatch.fnmatch(filename, cali_filename):\n",
        "            calibration_results.append(filename)\n",
        "    dfs = list()\n",
        "    for i, f in enumerate(calibration_results):\n",
        "        data = pd.read_csv(path + \"/\" + f)\n",
        "        #store fold name in df\n",
        "        data['Fold'] = re.findall(r'\\d+', f)[0]\n",
        "        #get metrics for computing calibration metrics\n",
        "        y_true = data['Truth'].to_numpy()\n",
        "        y_pred = data['Mean_Value'].to_numpy()\n",
        "        y_std = data['STD_Value'].to_numpy()\n",
        "        #compute calibration metrics\n",
        "        sess_sharp.append(uct.metrics_calibration.sharpness(y_std))\n",
        "        sess_ma.append(uct.metrics_calibration.miscalibration_area(y_pred, y_std, y_true))\n",
        "        dfs.append(data)\n",
        "    #concat all dfs to one df\n",
        "    all_calibration_df = pd.concat(dfs, ignore_index=True)\n",
        "    # get all calib result files into one df\n",
        "    uncert_results = []\n",
        "    for filename in os.listdir(path):\n",
        "        if fnmatch.fnmatch(filename, uncert_filename):\n",
        "            uncert_results.append(filename)\n",
        "    dfs1 = list()\n",
        "    for i, f in enumerate(uncert_results):\n",
        "        data = pd.read_csv(path + \"/\" + f)\n",
        "        # store fold name in df\n",
        "        data['Fold'] = re.findall(r'\\d+', f)[0]\n",
        "        fold.append(re.findall(r'\\d+', f)[0])\n",
        "        #compute fold specific metrics\n",
        "        sess_mae.append(np.mean(data[\"Absolute Error\"]))\n",
        "        sess_max_ae.append(np.max(data[\"Absolute Error\"]))\n",
        "        all_pred = data[\"Predictive_Mean\"].to_numpy()\n",
        "        y_test = data[\"Truth\"].to_numpy()\n",
        "        #compute bias + variance of folds\n",
        "        main_predictions = np.mean(all_pred, axis=0)\n",
        "        sess_bias.append(np.sum((main_predictions - y_test) ** 2) / y_test.size)\n",
        "        sess_var.append(np.sum((main_predictions - all_pred) ** 2) / all_pred.size)\n",
        "        dfs1.append(data)\n",
        "    # get all uncertainty result files into one df\n",
        "    all_uncert_df = pd.concat(dfs1, ignore_index=True)\n",
        "    #store fold specific metrics in dict to make into df\n",
        "    fold_specific_metrics = []\n",
        "    for i, session in enumerate(fold):\n",
        "      session_metrics = {\n",
        "          \"Fold\": fold[i],\n",
        "          \"Session_MAE\": sess_mae[i],\n",
        "          \"Session_Max_AE\": sess_max_ae[i],\n",
        "          \"Session_Miscal_Area\": sess_ma[i],\n",
        "          \"Session_Sharpness\": sess_sharp[i],\n",
        "          \"Session_Bias\": sess_bias[i],\n",
        "          \"Session_Var\": sess_var[i]\n",
        "      }\n",
        "      fold_specific_metrics.append(session_metrics)\n",
        "    # store overall metrics in dict to make into df\n",
        "    overall_dict = {\n",
        "        \"Exp_Name\": path.split(\"/Exp/\",1)[1],\n",
        "        \"MAE\": np.mean(sess_mae),\n",
        "        \"MAE_std\": np.std(sess_mae),\n",
        "        \"Max_Abs_Error\": np.mean(sess_max_ae),\n",
        "        \"Max_Abs_Error_std\": np.std(sess_max_ae),\n",
        "        \"Miscal_Area\": np.mean(sess_ma),\n",
        "        \"Miscal_Area_std\": np.std(sess_ma),\n",
        "        \"Sharpness\": np.mean(sess_sharp),\n",
        "        \"Sharpness_std\": np.std(sess_sharp),\n",
        "        \"Bias\": np.mean(sess_bias),\n",
        "        \"Bias_std\": np.std(sess_bias),\n",
        "        \"Var\": np.mean(sess_var),\n",
        "        \"Var_std\": np.std(sess_var)\n",
        "    }\n",
        "    #convert list of dicts to df\n",
        "    df_session_specific = pd.DataFrame(fold_specific_metrics)\n",
        "    df_overall = pd.DataFrame([overall_dict])\n",
        "    #save both dfs are csv files\n",
        "    df_session_specific.to_csv(path + \"/\" + str(sess_spec_file_name), index=False)\n",
        "    df_overall.to_csv(path + \"/\" + str(overall_file_name), index=False)\n",
        "    return all_uncert_df, all_calibration_df, df_session_specific, df_overall\n",
        "\n",
        "def save_pickle(PATH, EXP_NAME, Name, pkl_list):\n",
        "    \"\"\"\n",
        "    Save list as pickle.\n",
        "\n",
        "    Args:\n",
        "        PATH: The path of the experiment.\n",
        "        EXP_NAME: The name of the experiment.\n",
        "        Name: The name of the file.\n",
        "        pkl_list: The list to pickle.\n",
        "    \"\"\"\n",
        "    outfile = open(PATH + \"/\" + EXP_NAME + \"/\" + Name, 'wb')\n",
        "    pickle.dump(pkl_list, outfile)\n",
        "    outfile.close()\n",
        "\n",
        "def repeating_values(x):\n",
        "    \"\"\"\n",
        "    Get the results from the unseen session data.\n",
        "\n",
        "    Args:\n",
        "        x: list of activities.\n",
        "\n",
        "    Returns:\n",
        "        start_indexes: list of indices where the activity type starts.\n",
        "        activity: the activity name related to that index.\n",
        "    \"\"\"\n",
        "    start_indexes = []\n",
        "    activity = []\n",
        "    cur_idx = 0\n",
        "    i = 1\n",
        "    while i < len(x):\n",
        "        if x[cur_idx] == x[i]:\n",
        "            start_indexes.append(cur_idx)\n",
        "            activity.append(x[cur_idx])\n",
        "            # Increase value of i up to the index where there is different value\n",
        "            while i < len(x):\n",
        "                if x[cur_idx] != x[i]:\n",
        "                    # Set cur_idx to the index of different value\n",
        "                    cur_idx = i\n",
        "                    i += 1\n",
        "                    break\n",
        "                i += 1\n",
        "        else:\n",
        "            cur_idx = i\n",
        "            i += 1\n",
        "    return start_indexes, activity\n",
        "\n",
        "def Negative_Log_Likelihood(y, distribution):\n",
        "    \"\"\"\n",
        "    Get NLL of an observation given a distribution.\n",
        "\n",
        "    Args:\n",
        "        y: an observation.\n",
        "        distribution: a fitted distribution.\n",
        "\n",
        "    Returns:\n",
        "        -distribution.log_prob(y): the negative log likelihood of y given distribution.\n",
        "    \"\"\"\n",
        "    return -distribution.log_prob(y)\n",
        "\n",
        "def normal_sp(params):\n",
        "    \"\"\"\n",
        "    Converts model outputs to Normal Distribution\n",
        "\n",
        "    Args:\n",
        "        params: list of parameters outputted from model.\n",
        "\n",
        "    Returns:\n",
        "        tfd.Normal: A normal distribution.\n",
        "    \"\"\"\n",
        "    return tfd.Normal(loc=params[:, 0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:, 1:2]))\n",
        "\n",
        "def normal_sp_predict(params):\n",
        "    \"\"\"\n",
        "    Converts model outputs to Normal Distribution\n",
        "\n",
        "    Args:\n",
        "        params: list of parameters outputted from model.\n",
        "\n",
        "    Returns:\n",
        "        tfd.Normal: A normal distribution.\n",
        "    \"\"\"\n",
        "    return tfd.Normal(loc=params[0], scale=1e-3 + tf.math.softplus(0.05 * params[1]))\n",
        "\n",
        "def train_model_LOSO(data, PATH, EXP_NAME, num_epochs, batch_sze, stratified, \n",
        "                     test_sze, num_predict_samples, band_pass_low, band_pass_high, \n",
        "                     band_pass_order, resample_fs, display, publication, MODEL_TYPE,\n",
        "                     SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                     GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, \n",
        "                     GLOBAL_DROPOUT, RECURRENT_UNITS, \n",
        "                     RECURRENT_DROPOUT, POOLING):\n",
        "    \"\"\"\n",
        "    Preprocesses data, trains the model, displays training performance, predicts values of unseen session,\n",
        "    displays the performance of uncertainty metrics. Repeats for each fold in dataset then displays overall\n",
        "    performance.\n",
        "\n",
        "    Args:\n",
        "        data: The raw data file.\n",
        "        PATH: The path of the experiment folder.\n",
        "        EXP_NAME: The experiment name.\n",
        "        num_epochs: The number of training epochs.\n",
        "        batch_sze: The batch size of the model.\n",
        "        stratified: Whether the train/val sets are stratified on activity type.\n",
        "        test_sze: The percentage of the data used for validation set.\n",
        "        num_predict_samples: The number of samples from the predictive distribution.\n",
        "        band_pass_low: The lower cutoff frequency of the bandpass filter.\n",
        "        band_pass_high: The higher cutoff frequency of the bandpass filter.\n",
        "        band_pass_order: The order of the bandpass filter.\n",
        "        resample_fs: The desired sampling frequency.\n",
        "        display: Whether to display inline during training.\n",
        "        publication: Whether to export as svg or png format.\n",
        "        MODEL_TYPE:  whether original or optimised model\n",
        "        SENSOR_FILTER_SIZE: The filter size of the Sensor Specific Convolutional block.\n",
        "        SENSOR_NUM_FILTER: The number of filters in the Sensor Specific Convolutional block.\n",
        "        SENSOR_DROPOUT: The dropout rate for the Sensor Specific Convolutional block.\n",
        "        GLOBAL_FILTER_SIZE: The filter size of the Global Convolutional block.\n",
        "        GLOBAL_NUM_FILTER: The number of filters in the Global Convolutional block.\n",
        "        GLOBAL_DROPOUT: The dropout rate for the Global Convolutional block.\n",
        "        RECURRENT_UNITS: The number of units in the Recurrent block.\n",
        "        RECURRENT_DROPOUT: The dropout rate for the Recurrent block.\n",
        "        POOLING: The pooling for the Convolutional blocks.\n",
        "    \"\"\"\n",
        "    # preprocess dataset into df\n",
        "    print(\"Preprocessing Dataset: \")\n",
        "    DATASET = preprocess_dataset(data, band_pass_low, band_pass_high, \n",
        "                                 band_pass_order, resample_fs)\n",
        "    unique_check_train = []\n",
        "    unique_check_val = []\n",
        "    unique_check_test = []\n",
        "    train_times = []\n",
        "    predict_times = []\n",
        "    path = PATH + str(EXP_NAME) + str(MODEL_TYPE)\n",
        "    SUBJECT = DATASET[\"Signal_ID\"].unique().tolist()\n",
        "    random.shuffle(SUBJECT, random.random)\n",
        "    model_print = 0\n",
        "    # for each fold in dataset\n",
        "    for fold, test_subject in enumerate(SUBJECT):\n",
        "        model_print += 1\n",
        "        # get train/val/test split\n",
        "        df_train, df_val, df_test = get_dataset_splits_loso(DATASET, stratified, \n",
        "                                                            test_sze, test_subject)\n",
        "\n",
        "        df_train[\"Dataset\"] = \"Train\"\n",
        "        df_val[\"Dataset\"] = \"Validation\"\n",
        "        df_test[\"Dataset\"] = \"Test\"\n",
        "        # get data in correct format for model\n",
        "        PPG_Train = np.stack(df_train['PPG'], axis=0)\n",
        "        ACC_Train = np.stack(df_train['ACC'], axis=0)\n",
        "        Truth_Train = np.stack(df_train['Truth'], axis=0)\n",
        "\n",
        "        PPG_Test = np.stack(df_test['PPG'], axis=0)\n",
        "        ACC_Test = np.stack(df_test['ACC'], axis=0)\n",
        "        Truth_Test = np.stack(df_test['Truth'], axis=0)\n",
        "\n",
        "        PPG_Val = np.stack(df_val['PPG'], axis=0)\n",
        "        ACC_Val = np.stack(df_val['ACC'], axis=0)\n",
        "        Truth_Val = np.stack(df_val['Truth'], axis=0)\n",
        "\n",
        "        # callbacks\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=70,\n",
        "                                              verbose=1, restore_best_weights=False)\n",
        "        tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
        "        modelname = path + '/fold' + str(fold) + '.h5'\n",
        "        rlronp = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                      factor=0.9, patience=5,\n",
        "                                                      verbose=1)\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            modelname,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1, mode='min')\n",
        "       \n",
        "            # init model & optimiser\n",
        "        if MODEL_TYPE == \"Original\": \n",
        "          model = make_DeepPulse_Org(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, \n",
        "                                SENSOR_DROPOUT, GLOBAL_FILTER_SIZE, \n",
        "                                GLOBAL_NUM_FILTER, GLOBAL_DROPOUT, \n",
        "                                RECURRENT_UNITS, RECURRENT_DROPOUT, POOLING)\n",
        "        elif MODEL_TYPE == \"Optimised\":\n",
        "          model = make_DeepPulse_Opt(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, \n",
        "                                  SENSOR_DROPOUT, GLOBAL_FILTER_SIZE, \n",
        "                                  GLOBAL_NUM_FILTER, GLOBAL_DROPOUT, \n",
        "                                  RECURRENT_UNITS, RECURRENT_DROPOUT, POOLING)\n",
        "        callbacks = [es, rlronp, checkpoint_callback, tqdm_callback]\n",
        "        OPT = tf.optimizers.Nadam()\n",
        "        model.compile(loss=Negative_Log_Likelihood, optimizer=OPT, metrics=['mae'])\n",
        "        # train model\n",
        "        start_training_time = time.time()\n",
        "        print(\"Fit model on training data fold \", fold, \"/\", len(SUBJECT))\n",
        "        history = model.fit(\n",
        "            [PPG_Train, ACC_Train],\n",
        "            Truth_Train,\n",
        "            batch_size=batch_sze,\n",
        "            epochs=num_epochs,\n",
        "            validation_data=([PPG_Val, ACC_Val], Truth_Val),\n",
        "            verbose=0,\n",
        "            callbacks=callbacks)\n",
        "        time.sleep(2)\n",
        "        output.clear()\n",
        "        # load best performing weights\n",
        "        model.load_weights(modelname)\n",
        "        # use trained model to predict on unseen session\n",
        "        eval_history = model.evaluate(\n",
        "            [PPG_Test, ACC_Test],\n",
        "            Truth_Test,\n",
        "            batch_size=batch_sze, verbose=0,\n",
        "            callbacks=[tqdm_callback])\n",
        "        end_training_time = time.time()\n",
        "        train_time = end_training_time - start_training_time\n",
        "        output.clear()\n",
        "        # store windows used in each set of data to perform similarity analysis\n",
        "        train_set_uiq = list(df_train.Signal_ID.astype(str).str.cat(df_train.Window_ID.astype(str), sep='w'))\n",
        "        val_set_uiq = list(df_val.Signal_ID.astype(str).str.cat(df_val.Window_ID.astype(str), sep='w'))\n",
        "        test_set_uiq = list(df_test.Signal_ID.astype(str).str.cat(df_test.Window_ID.astype(str), sep='w'))\n",
        "        unique_check_train.append(train_set_uiq)\n",
        "        unique_check_val.append(val_set_uiq)\n",
        "        unique_check_test.append(test_set_uiq)\n",
        "        # save model arch image\n",
        "        if model_print == 1:\n",
        "          if MODEL_TYPE == \"Original\":\n",
        "            tf.keras.utils.plot_model(\n",
        "                    make_DeepPulse_Org(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, \n",
        "                                SENSOR_DROPOUT, GLOBAL_FILTER_SIZE, \n",
        "                                GLOBAL_NUM_FILTER, GLOBAL_DROPOUT, \n",
        "                                RECURRENT_UNITS, RECURRENT_DROPOUT, POOLING),\n",
        "                to_file=path + \"/model.png\",\n",
        "                show_shapes=True,\n",
        "            )\n",
        "          elif MODEL_TYPE == \"Optimised\":\n",
        "            tf.keras.utils.plot_model(\n",
        "                    make_DeepPulse_Opt(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, \n",
        "                                  SENSOR_DROPOUT, GLOBAL_FILTER_SIZE, \n",
        "                                  GLOBAL_NUM_FILTER, GLOBAL_DROPOUT, \n",
        "                                  RECURRENT_UNITS, RECURRENT_DROPOUT, POOLING),\n",
        "                to_file=path + \"/model.png\",\n",
        "                show_shapes=True,\n",
        "            )\n",
        "            # save experiment specific parameters\n",
        "            with open(path + \"/variables.txt\", \"w\") as f:\n",
        "                f.write(\"PREPROCESSING VARIABLES:\" + \"\\n \\n\" + \"BAND_PASS_LOW = \"\n",
        "                 + str(band_pass_low) + \"\\n\" + \"BAND_PASS_HIGH = \" + \n",
        "                 str(band_pass_high) + \"\\n\" + \"BAND_PASS_ORDER = \" + \n",
        "                 str(band_pass_order) + \"\\n\" + \"RESAMPLE_FS = \" + str(resample_fs)\n",
        "                  + \"TRAINING VARIABLES:\" + \"\\n \\n\"  + \"CV = LOSO \\n\"  + \"NUM_EPOCHS = \" + \n",
        "                  str(num_epochs) + \"\\n\" \n",
        "                + \"BATCH_SIZE = \" + str(batch_sze) + \"\\n\" \n",
        "                + \"STRATIFIED = \" + str(stratified) + \"\\n\" + \"VAL_SET_SIZE = \" \n",
        "                + str(test_sze) + \"\\n\" + \"NUM_PREDICT_SAMPLES = \" + \n",
        "                str(num_predict_samples)+ \"\\n\" + \"ARCHITECTURAL VARIABLES:\" + \n",
        "                \"\\n \\n\" +  \"MODEL_TYPE = \" + str(MODEL_TYPE) + \"\\n\" +\n",
        "                \"SENSOR_FILTER_SIZE = \" + str(SENSOR_FILTER_SIZE) + \"\\n\" + \n",
        "                \"SENSOR_NUM_FILTER = \" + str(SENSOR_NUM_FILTER) +\n",
        "                 \"\\n\"+ \"SENSOR_DROPOUT = \"  + str(SENSOR_DROPOUT) + \n",
        "                + \"\\n\" + \"GLOBAL_FILTER_SIZE = \" + str(GLOBAL_FILTER_SIZE)\n",
        "                  + \"\\n\"+ \"GLOBAL_NUM_FILTER = \" + str(GLOBAL_NUM_FILTER) + \"\\n\"+ \n",
        "                \"GLOBAL_DROPOUT = \" + str(GLOBAL_DROPOUT) + \n",
        "                \"\\n\"+ \"RECURRENT_UNITS = \" + str(RECURRENT_UNITS)\n",
        "                  + \"\\n\" + \"RECURRENT_DROPOUT = \" + str(RECURRENT_DROPOUT) \n",
        "                  + \"\\n\" + \"POOLING = \" + str(POOLING))\n",
        "                f.close()\n",
        "        start_predict_time = time.time()\n",
        "        # get training graph\n",
        "        df_result = pd.concat([df_train, df_val, df_test]).reset_index(drop=True)\n",
        "      \n",
        "        display_training_performance(df_result, history, eval_history, fold, path, display, publication)\n",
        "        # get calibration graph\n",
        "        calibration_df, uncert_df = get_test_results(model, df_test, fold, modelname, num_predict_samples)\n",
        "        display_calibration_performance(calibration_df, uncert_df, fold, path, display, publication)\n",
        "        end_predict_time = time.time()\n",
        "        predict_time = end_predict_time - start_predict_time\n",
        "        train_times.append(train_time)\n",
        "        predict_times.append(predict_time)\n",
        "        with open(path + \"/times.txt\", \"a\") as f:\n",
        "          f.write(\"FOLD: \" + str(fold)+ \"\\n\"+ \"Train Time: \" + str(train_time)+ \"\\n\"\n",
        "          + \"Predict Time: \" + str(predict_time)+ \"\\n\")\n",
        "          f.close()\n",
        "        # delete fold data\n",
        "        del history\n",
        "        del eval_history\n",
        "        del model\n",
        "  \n",
        "        gc.collect()\n",
        "    save_pickle(PATH, EXP_NAME, \"unique_check_train\", unique_check_train)\n",
        "    save_pickle(PATH, EXP_NAME, \"unique_check_val\", unique_check_val)\n",
        "    save_pickle(PATH, EXP_NAME, \"unique_check_test\", unique_check_test)\n",
        "    # get overall graph\n",
        "    display_overall_performance(path, unique_check_train, unique_check_val, unique_check_test, display, publication)\n",
        "    display_overall_performance(path, unique_check_train, unique_check_val, unique_check_test, display, publication)\n",
        "\n",
        "def average_len(l):\n",
        "    \"\"\"\n",
        "    Computes the average length of a list of lists\n",
        "    Taken from: https://stackoverflow.com/a/15772649\n",
        "\n",
        "    Args:\n",
        "        l: list of lists.\n",
        "\n",
        "    Returns:\n",
        "        sum(map(len, l))/float(len(l)): Average length of a list of lists.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum(map(len, l))/float(len(l))\n",
        "    "
      ],
      "metadata": {
        "id": "RIXc1k5T9Hjk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY-A108XPzII"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KRI0cjgMelzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8adca63-a6ed-4c21-a6de-7586ee30c617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DeepPulse\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " PPG_Sigs (InputLayer)          [(None, 512, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " ACC_Sigs (InputLayer)          [(None, 512, 3)]     0           []                               \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_conv (Conv1D)     (None, 512, 64)      1024        ['PPG_Sigs[0][0]']               \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_conv (Conv1D)     (None, 512, 64)      3072        ['ACC_Sigs[0][0]']               \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_relu (ReLU)       (None, 512, 64)      0           ['PPG_SENSOR_1_conv[0][0]']      \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_relu (ReLU)       (None, 512, 64)      0           ['ACC_SENSOR_1_conv[0][0]']      \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_batch_norm (Batch  (None, 512, 64)     256         ['PPG_SENSOR_1_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_batch_norm (Batch  (None, 512, 64)     256         ['ACC_SENSOR_1_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_max_pool (MaxPool  (None, 256, 64)     0           ['PPG_SENSOR_1_batch_norm[0][0]']\n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_max_pool (MaxPool  (None, 256, 64)     0           ['ACC_SENSOR_1_batch_norm[0][0]']\n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_dropout (Dropout)  (None, 256, 64)     0           ['PPG_SENSOR_1_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_dropout (Dropout)  (None, 256, 64)     0           ['ACC_SENSOR_1_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_conv (Conv1D)     (None, 256, 64)      65536       ['PPG_SENSOR_1_dropout[0][0]']   \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_conv (Conv1D)     (None, 256, 64)      65536       ['ACC_SENSOR_1_dropout[0][0]']   \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_relu (ReLU)       (None, 256, 64)      0           ['PPG_SENSOR_2_conv[0][0]']      \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_relu (ReLU)       (None, 256, 64)      0           ['ACC_SENSOR_2_conv[0][0]']      \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_batch_norm (Batch  (None, 256, 64)     256         ['PPG_SENSOR_2_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_batch_norm (Batch  (None, 256, 64)     256         ['ACC_SENSOR_2_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_max_pool (MaxPool  (None, 128, 64)     0           ['PPG_SENSOR_2_batch_norm[0][0]']\n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_max_pool (MaxPool  (None, 128, 64)     0           ['ACC_SENSOR_2_batch_norm[0][0]']\n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_dropout (Dropout)  (None, 128, 64)     0           ['PPG_SENSOR_2_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_dropout (Dropout)  (None, 128, 64)     0           ['ACC_SENSOR_2_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " MERGE (Concatenate)            (None, 128, 128)     0           ['PPG_SENSOR_2_dropout[0][0]',   \n",
            "                                                                  'ACC_SENSOR_2_dropout[0][0]']   \n",
            "                                                                                                  \n",
            " GLOBAL_1_conv (Conv1D)         (None, 128, 128)     262144      ['MERGE[0][0]']                  \n",
            "                                                                                                  \n",
            " GLOBAL_1_relu (ReLU)           (None, 128, 128)     0           ['GLOBAL_1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " GLOBAL_1_batch_norm (BatchNorm  (None, 128, 128)    512         ['GLOBAL_1_relu[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " GLOBAL_1_max_pool (MaxPooling1  (None, 64, 128)     0           ['GLOBAL_1_batch_norm[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " GLOBAL_1_dropout (Dropout)     (None, 64, 128)      0           ['GLOBAL_1_max_pool[0][0]']      \n",
            "                                                                                                  \n",
            " GLOBAL_2_conv (Conv1D)         (None, 64, 128)      262144      ['GLOBAL_1_dropout[0][0]']       \n",
            "                                                                                                  \n",
            " GLOBAL_2_relu (ReLU)           (None, 64, 128)      0           ['GLOBAL_2_conv[0][0]']          \n",
            "                                                                                                  \n",
            " GLOBAL_2_batch_norm (BatchNorm  (None, 64, 128)     512         ['GLOBAL_2_relu[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " GLOBAL_2_dropout (Dropout)     (None, 64, 128)      0           ['GLOBAL_2_batch_norm[0][0]']    \n",
            "                                                                                                  \n",
            " RECURRENT_1_bidirectional (Bid  (None, 64, 64)      41216       ['GLOBAL_2_dropout[0][0]']       \n",
            " irectional)                                                                                      \n",
            "                                                                                                  \n",
            " RECURRENT_2_bidirectional (Bid  (None, 64, 64)      24832       ['RECURRENT_1_bidirectional[0][0]\n",
            " irectional)                                                     ']                               \n",
            "                                                                                                  \n",
            " PREDICT_conv (Conv1D)          (None, 64, 1)        1024        ['RECURRENT_2_bidirectional[0][0]\n",
            "                                                                 ']                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 64)           0           ['PREDICT_conv[0][0]']           \n",
            "                                                                                                  \n",
            " Dense_1 (Dense)                (None, 2)            130         ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " Distribution_1 (DistributionLa  ((None, 1),         0           ['Dense_1[0][0]']                \n",
            " mbda)                           (None, 1))                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 728,706\n",
            "Trainable params: 727,682\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "make_DeepPulse_Org(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                   GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, \n",
        "                   GLOBAL_DROPOUT, RECURRENT_UNITS, \n",
        "                   RECURRENT_DROPOUT, POOLING).summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_DeepPulse_Opt(SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                   GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, \n",
        "                   GLOBAL_DROPOUT, RECURRENT_UNITS, \n",
        "                   RECURRENT_DROPOUT, POOLING).summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-PrP5UhFd0I",
        "outputId": "17b2cc87-bfcc-4649-d08c-32410aac3935"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DeepPulse\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " PPG_Sigs (InputLayer)          [(None, 512, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " ACC_Sigs (InputLayer)          [(None, 512, 3)]     0           []                               \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_conv (Conv1D)     (None, 512, 64)      1024        ['PPG_Sigs[0][0]']               \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_conv (Conv1D)     (None, 512, 64)      3072        ['ACC_Sigs[0][0]']               \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_relu (ReLU)       (None, 512, 64)      0           ['PPG_SENSOR_1_conv[0][0]']      \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_relu (ReLU)       (None, 512, 64)      0           ['ACC_SENSOR_1_conv[0][0]']      \n",
            "                                                                                                  \n",
            " PPG_SENSOR_1_batch_norm (Batch  (None, 512, 64)     256         ['PPG_SENSOR_1_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " ACC_SENSOR_1_batch_norm (Batch  (None, 512, 64)     256         ['ACC_SENSOR_1_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_conv (Conv1D)     (None, 512, 64)      65536       ['PPG_SENSOR_1_batch_norm[0][0]']\n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_conv (Conv1D)     (None, 512, 64)      65536       ['ACC_SENSOR_1_batch_norm[0][0]']\n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_relu (ReLU)       (None, 512, 64)      0           ['PPG_SENSOR_2_conv[0][0]']      \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_relu (ReLU)       (None, 512, 64)      0           ['ACC_SENSOR_2_conv[0][0]']      \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_batch_norm (Batch  (None, 512, 64)     256         ['PPG_SENSOR_2_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_batch_norm (Batch  (None, 512, 64)     256         ['ACC_SENSOR_2_relu[0][0]']      \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_max_pool (Average  (None, 256, 64)     0           ['PPG_SENSOR_2_batch_norm[0][0]']\n",
            " Pooling1D)                                                                                       \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_max_pool (Average  (None, 256, 64)     0           ['ACC_SENSOR_2_batch_norm[0][0]']\n",
            " Pooling1D)                                                                                       \n",
            "                                                                                                  \n",
            " PPG_SENSOR_2_dropout (Dropout)  (None, 256, 64)     0           ['PPG_SENSOR_2_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " ACC_SENSOR_2_dropout (Dropout)  (None, 256, 64)     0           ['ACC_SENSOR_2_max_pool[0][0]']  \n",
            "                                                                                                  \n",
            " MERGE (Concatenate)            (None, 256, 128)     0           ['PPG_SENSOR_2_dropout[0][0]',   \n",
            "                                                                  'ACC_SENSOR_2_dropout[0][0]']   \n",
            "                                                                                                  \n",
            " GLOBAL_1_conv (Conv1D)         (None, 256, 128)     262144      ['MERGE[0][0]']                  \n",
            "                                                                                                  \n",
            " GLOBAL_1_relu (ReLU)           (None, 256, 128)     0           ['GLOBAL_1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " GLOBAL_1_batch_norm (BatchNorm  (None, 256, 128)    512         ['GLOBAL_1_relu[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " GLOBAL_1_max_pool (AveragePool  (None, 128, 128)    0           ['GLOBAL_1_batch_norm[0][0]']    \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " GLOBAL_1_dropout (Dropout)     (None, 128, 128)     0           ['GLOBAL_1_max_pool[0][0]']      \n",
            "                                                                                                  \n",
            " GLOBAL_2_conv (Conv1D)         (None, 128, 128)     262144      ['GLOBAL_1_dropout[0][0]']       \n",
            "                                                                                                  \n",
            " GLOBAL_2_relu (ReLU)           (None, 128, 128)     0           ['GLOBAL_2_conv[0][0]']          \n",
            "                                                                                                  \n",
            " GLOBAL_2_batch_norm (BatchNorm  (None, 128, 128)    512         ['GLOBAL_2_relu[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " GLOBAL_2_max_pool (AveragePool  (None, 64, 128)     0           ['GLOBAL_2_batch_norm[0][0]']    \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " GLOBAL_2_dropout (Dropout)     (None, 64, 128)      0           ['GLOBAL_2_max_pool[0][0]']      \n",
            "                                                                                                  \n",
            " RECURRENT_1_bidirectional (Bid  (None, 64, 64)      41216       ['GLOBAL_2_dropout[0][0]']       \n",
            " irectional)                                                                                      \n",
            "                                                                                                  \n",
            " RECURRENT_1_layer_norm (LayerN  (None, 64, 64)      128         ['RECURRENT_1_bidirectional[0][0]\n",
            " ormalization)                                                   ']                               \n",
            "                                                                                                  \n",
            " RECURRENT_2_bidirectional (Bid  (None, 64)          24832       ['RECURRENT_1_layer_norm[0][0]'] \n",
            " irectional)                                                                                      \n",
            "                                                                                                  \n",
            " RECURRENT_2_layer_norm (LayerN  (None, 64)          128         ['RECURRENT_2_bidirectional[0][0]\n",
            " ormalization)                                                   ']                               \n",
            "                                                                                                  \n",
            " Dense_1 (Dense)                (None, 2)            130         ['RECURRENT_2_layer_norm[0][0]'] \n",
            "                                                                                                  \n",
            " Distribution_1 (DistributionLa  ((None, 1),         0           ['Dense_1[0][0]']                \n",
            " mbda)                           (None, 1))                                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 727,938\n",
            "Trainable params: 726,914\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD0XBIc6BSLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "c84a76bbd73f4f7a9c8e86e49fece523",
            "e05158ff278546c5b38e7ada3ac30169",
            "fd8d72761f444ad6b48587693e1d1881",
            "cfcf5f0854f541a78e0c9bc1432ce7e0",
            "1bfd3951d5b546ecb30a6f02d3c3858e",
            "1684a0a064354b9788ac4c2f752e631e",
            "70e4a1ecf8224eb5aa3c9a7c060e1882",
            "cd7b8c7d404c493e87ffb40376bc2cb6",
            "f031e0f94b274957ab98ff76fbd9eaa3",
            "02ae90c4299c4de8869b2c38cdc7c5c4",
            "399bd67634444917a9759bf079d926c2",
            "8b4e9291800c46c383762deff64a6aa3",
            "421d36ca84d54721a86e95ab58d220e0",
            "94a6a9c6af104f3e8ce6e495edd21874",
            "e2e4c0ea18ce46f795fd185a3c9df85a",
            "f8af795229e94bc79e2089c8bfa96e53",
            "da30d6a04b4949d38970806c1784f3c7",
            "a7d63a5a8c39496aa1f54e1c03c6b94b",
            "d1937b2465d94c018f9dbc432c468172",
            "8bc8591b82e84fb684bbc9b0d5e2a6fe",
            "efd681939e984c79b8e984c3651a4f8f",
            "65cbba7012f244e8ac9a290835ab5801",
            "7e711988f46648ee8707fc2bb365d473",
            "3bdf7d4b0f454d1998baf076df4a965b",
            "1d024d1eadaa4d2cbbaa13687168c42d",
            "9d4f8682d55e48c399fda7757e92a07b",
            "6fd33010743f45dba83e68a91b025098",
            "5d666b5d07b942aaaeb0fa85ce24c2e6",
            "a427626334ba4ffbb60b80fb7b92d328",
            "24e1b3cc450842bc957d4299b02c4ecd",
            "1fd0301408074c83ba52073e9f3cefde",
            "8fb452cadfd54bc8900469fb649c1bf4",
            "d33529596051412dac37e6dc0efb5b3d"
          ]
        },
        "outputId": "53183754-9bd9-4a90-c225-36ca669c1c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing Dataset: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c84a76bbd73f4f7a9c8e86e49fece523",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Sessions:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data fold  0 / 15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b4e9291800c46c383762deff64a6aa3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training:   0%|           0/200 ETA: ?s,  ?epochs/s"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e711988f46648ee8707fc2bb365d473",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0/1355           ETA: ?s - "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "for model in MODEL_TYPE:\n",
        "  for idx, dataset in enumerate(DATA):\n",
        "    train_model_LOSO(dataset, PATH, EXP_NAME[idx], NUM_EPOCHS, BATCH_SIZE, STRATIFIED, \n",
        "                TEST_SIZE, NUM_PREDICT_SAMPLES, BAND_PASS_LOW, BAND_PASS_HIGH, \n",
        "                BAND_PASS_ORDER, RESAMPLE_FS, DISPLAY, PUBLICATION, model, \n",
        "                SENSOR_FILTER_SIZE, SENSOR_NUM_FILTER, SENSOR_DROPOUT, \n",
        "                GLOBAL_FILTER_SIZE, GLOBAL_NUM_FILTER, GLOBAL_DROPOUT, \n",
        "                RECURRENT_UNITS, RECURRENT_DROPOUT, POOLING)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTOGUC4rHNR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO9TLSM0YQqAJW4Ct6SizUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c84a76bbd73f4f7a9c8e86e49fece523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e05158ff278546c5b38e7ada3ac30169",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd8d72761f444ad6b48587693e1d1881",
              "IPY_MODEL_cfcf5f0854f541a78e0c9bc1432ce7e0",
              "IPY_MODEL_1bfd3951d5b546ecb30a6f02d3c3858e"
            ]
          }
        },
        "e05158ff278546c5b38e7ada3ac30169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd8d72761f444ad6b48587693e1d1881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1684a0a064354b9788ac4c2f752e631e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Sessions: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70e4a1ecf8224eb5aa3c9a7c060e1882"
          }
        },
        "cfcf5f0854f541a78e0c9bc1432ce7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd7b8c7d404c493e87ffb40376bc2cb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f031e0f94b274957ab98ff76fbd9eaa3"
          }
        },
        "1bfd3951d5b546ecb30a6f02d3c3858e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_02ae90c4299c4de8869b2c38cdc7c5c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15/15 [00:30&lt;00:00,  1.94s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_399bd67634444917a9759bf079d926c2"
          }
        },
        "1684a0a064354b9788ac4c2f752e631e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70e4a1ecf8224eb5aa3c9a7c060e1882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd7b8c7d404c493e87ffb40376bc2cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f031e0f94b274957ab98ff76fbd9eaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02ae90c4299c4de8869b2c38cdc7c5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "399bd67634444917a9759bf079d926c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b4e9291800c46c383762deff64a6aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_421d36ca84d54721a86e95ab58d220e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94a6a9c6af104f3e8ce6e495edd21874",
              "IPY_MODEL_e2e4c0ea18ce46f795fd185a3c9df85a",
              "IPY_MODEL_f8af795229e94bc79e2089c8bfa96e53"
            ]
          }
        },
        "421d36ca84d54721a86e95ab58d220e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "94a6a9c6af104f3e8ce6e495edd21874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da30d6a04b4949d38970806c1784f3c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Training:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7d63a5a8c39496aa1f54e1c03c6b94b"
          }
        },
        "e2e4c0ea18ce46f795fd185a3c9df85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1937b2465d94c018f9dbc432c468172",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bc8591b82e84fb684bbc9b0d5e2a6fe"
          }
        },
        "f8af795229e94bc79e2089c8bfa96e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efd681939e984c79b8e984c3651a4f8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/200 ETA: ?s,  ?epochs/s",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65cbba7012f244e8ac9a290835ab5801"
          }
        },
        "da30d6a04b4949d38970806c1784f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7d63a5a8c39496aa1f54e1c03c6b94b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1937b2465d94c018f9dbc432c468172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bc8591b82e84fb684bbc9b0d5e2a6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efd681939e984c79b8e984c3651a4f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65cbba7012f244e8ac9a290835ab5801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e711988f46648ee8707fc2bb365d473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3bdf7d4b0f454d1998baf076df4a965b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1d024d1eadaa4d2cbbaa13687168c42d",
              "IPY_MODEL_9d4f8682d55e48c399fda7757e92a07b",
              "IPY_MODEL_6fd33010743f45dba83e68a91b025098"
            ]
          }
        },
        "3bdf7d4b0f454d1998baf076df4a965b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "1d024d1eadaa4d2cbbaa13687168c42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d666b5d07b942aaaeb0fa85ce24c2e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "1351/1355",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a427626334ba4ffbb60b80fb7b92d328"
          }
        },
        "9d4f8682d55e48c399fda7757e92a07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_24e1b3cc450842bc957d4299b02c4ecd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1355,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1351,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fd0301408074c83ba52073e9f3cefde"
          }
        },
        "6fd33010743f45dba83e68a91b025098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fb452cadfd54bc8900469fb649c1bf4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " ETA: 00:00s - loss: 336.5019 - mae: 18.8569",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d33529596051412dac37e6dc0efb5b3d"
          }
        },
        "5d666b5d07b942aaaeb0fa85ce24c2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a427626334ba4ffbb60b80fb7b92d328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24e1b3cc450842bc957d4299b02c4ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fd0301408074c83ba52073e9f3cefde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fb452cadfd54bc8900469fb649c1bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d33529596051412dac37e6dc0efb5b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}